<!doctype html><html>
    <head>
      <meta http-equiv="content-type" content="text/html; charset=utf-8">
      <title>Intro to tensorflow</title>
      <meta name="generator" content="CherryTree">
      <link rel="stylesheet" href="res/styles3.css" type="text/css" />
    </head>
    <body><div class='page'><h1 class='title'>Intro to tensorflow</h1><br/>Hello World of Neural networks <br /><br />This is written using Python and TensorFlow and an API in TensorFlow called keras. <br />Keras makes it really easy to define neural networks. <br />A neural network is basically a set of functions which can learn patterns.<br /><br />The simplest possible neural network is one that has only one neuron in it, and that&#39;s what this line of code does. <br />In keras, you use the word dense to define a layer of connected neurons. <br />There&#39;s only one dense here. <br />So there&#39;s only one layer and there&#39;s only one unit in it, so it&#39;s a single neuron. <br />Successive layers are defined in sequence, hence the word sequential. <br /><br />You define the shape of what&#39;s input to the neural network in the first and in this case the only layer, and you can see that our input shape is super simple. It&#39;s just one value.<br /><br />There are two function roles that you should be aware of though and these are loss functions and optimizers. <br />This code defines them.<br />The neural network has no idea of the relationship between X and Y, so it makes a guess. <br />Say it guesses Y equals 10X minus 10. It will then use the data that it knows about, that&#39;s the set of Xs and Ys that we&#39;ve already seen <br />to measure how good or how bad its guess was. <br />The loss function measures this and then gives the data to the optimizer which figures out the next guess. <br />So the optimizer thinks about how good or how badly the guess was done using the data from the loss function. <br />Then the logic is that each guess should be better than the one before. <br />As the guesses get better and better, an accuracy approaches 100 percent, the term convergence is used. <br /><br />In this case, the loss is mean squared error and the optimizer is SGD which stands for stochastic gradient descent. <br />If you want to learn more about these particular functions, as well as the other options that might be better in other scenarios, check out the TensorFlow documentation. <br /><br />Our next step is to represent the known data. <br />These are the Xs and the Ys that you saw earlier. <br />The np.array is using a Python library called numpy that makes data representation particularly enlists much easier. <br />So here you can see we have one list for the Xs and another one for the Ys. <br />The training takes place in the fit command. Here we&#39;re asking the model to figure out how to fit the X values to the Y values. <br />The epochs equals 500 value means that it will go through the training loop 500 times.<br /><br /> <br />Now you might think it would return 19 because after all Y equals 2X minus 1, and you think it should be 19. <br />But when you try this in the workbook yourself, you&#39;ll see that it will return a value very close to 19 but not exactly 19. <br />Now why do you think that would be? <br />Ultimately there are two main reasons. <br />The first is that you trained it using very little data. There&#39;s only six points. <br />Those six points are linear but there&#39;s no guarantee that for every X, the relationship will be Y equals 2X minus 1. <br />There&#39;s a very high probability that Y equals 19 for X equals 10, but the neural network isn&#39;t positive. <br />So it will figure out a realistic value for Y. That&#39;s the second main reason. <br />When using neural networks, as they try to figure out the answers for everything, they deal in probability. <br />You&#39;ll see that a lot and you&#39;ll have to adjust how you handle answers to fit. Keep that in mind as you work through the code. <br /><br />Working through “Hello World” in Tf <br /><a href=""><img src="images/48-1.png" alt="images/48-1.png" /></a><br /><br /><br /><br />research.google.com/seedbank imp<br /><br /><br />Computer Vision<br /> <br />These images have been scaled down to 28 by 28 pixels. <br />Now usually, the smaller the better because the computer has less processing to do. <br />But of course, you need to retain enough information to be sure that the features and the object can still be distinguished.<br />But of course, you need to retain enough information to be sure that the features and the object can still be distinguished.<br />So this size does seem to be ideal, and it makes it great for training a neural network. <br />The images are also in gray scale, so the amount of information is also reduced. <br />Each pixel can be represented in values from zero to 255 and so it&#39;s only one byte per pixel. <br />With 28 by 28 pixels in an image, only 784 bytes are needed to store the entire image.<br /><br /><a href="https://github.com/zalandoresearch/fashion-mnist">https://github.com/zalandoresearch/fashion-mnist</a><br /><a href=""><img src="images/48-2.png" alt="images/48-2.png" /></a><br /><br />So in the Fashion-MNIST data set, 60,000 of the 70,000 images are used to train the network, and then 10,000 image scan be used to test just how <br />good or how bad it is performing.<br /><br /><br /><a href="https://ai.google/responsibilities/responsible-ai-practices/">https://ai.google/responsibilities/responsible-ai-practices/</a><br /><br />The last layer has 10 neurons in it because we have ten classes of clothing in the dataset. <br />They should always match. The first layer is a flatten layer with the input shaping 28 by 28. <br />Now, if you remember our images are 28 by 28, so we&#39;re specifying that this is the shape that we should expect the data to be in. <br />Flatten takes this 28 by 28 square and turns it into a simple linear array. <br />The interesting stuff happens in the middle layer, sometimes also called a hidden layer. <br />This is a 128 neurons in it, and I&#39;d like you to think about these as variables in a function. <br />Maybe call them x1, x2 x3, etc. Now, there exists a rule that incorporates all of these that turns the 784 values of an ankle boot into the value nine, and similar for all of the other 70,000. It&#39;s too complex a function for you to see by mapping the images yourself, but that&#39;s what a neural net does. <br />So, for example, if you then say the function was y equals w1 times x1, plus w2 times x2, plus w3 times x3, all the way up to a w128 times x128. <br />By figuring out the values of w, then y will be nine, when you have the input value of the shoe. <br />You&#39;ll see that it&#39;s doing something very, very similar to what we did earlier when we figured out y equals 2x minus one. <br />In that case the two, was the weight of x. So, I&#39;m saying y equals w1 times x1, etc.<br /><a href=""><img src="images/48-3.png" alt="images/48-3.png" /></a><br /><a href=""><img src="images/48-4.png" alt="images/48-4.png" /></a><br /> <br />Let&#39;s now work through a workbook that has all of the code to do that. <br />You&#39;ll then go through this workbook yourself and if you want you can try some exercises. <br />Let&#39;s start by importing TensorFlow. I&#39;m going to get the fashion MNIST data using tf.kares.datasets. <br />By calling the load data method, I get training data and labels as well as test data and labels. <br />For more details on these, check back to the previous video. The data for a particular image is a grid of values from zero to 255 with pixel Grayscale values. Using matplotlib, I can plot these as an image to make it easier to inspect. <br />I can also print out the raw values so we can see what they look like. Here you can see the raw values for the pixel numbers from zero to 255, and here you can see the actual image. That was for the first image in the array. Let&#39;s take a look at the image at index 42 instead, and we can see the different pixel values and the actual graphic. <br />Our image has values from zero to 255, but neural networks work better with normalized data. <br />So, let&#39;s change it to between zero and one simply by dividing every value by 255. <br />In Python, you can actually divide an entire array with one line of code like this. So now we design our model. As explained earlier, there&#39;s an input layer in the shape of the data and an output layer in the shape of the classes, and one hidden layer that tries to figure out the roles between them. <br />Now we compile the model to finding the loss function and the optimizer, and the goal of these is as before, to make a guess as to what the relationship is between the input data and the output data, measure how well or how badly it did using the loss function, use the optimizer to generate a new gas and repeat. We can then try to fit the training images to the training labels. <br />We&#39;ll just do it for five epochs to be quick. We spend about 25 seconds training it over five epochs and we end up with a loss of about 0.29. <br />That means it&#39;s pretty accurate in guessing the relationship between the images and their labels. <br />That&#39;s not great, but considering it was done in just 25 seconds with a very basic neural network, it&#39;s not bad either. <br />But a better measure of performance can be seen by trying the test data. <br />These are images that the network has not yet seen. You would expect performance to be worse, but if it&#39;s much worse, you have a problem. As you can see, it&#39;s about 0.345 loss, meaning it&#39;s a little bit less accurate on the test set. It&#39;s not great either, but we know we&#39;re doing something right. <br />Your job now is to go through the workbook, try the exercises and see by tweaking the parameters on the neural network or changing the epochs, if there&#39;s a way for you to get it above 0.71 loss accuracy on training data and 0.66 accuracy on test data,<br /><a href=""><img src="images/48-5.png" alt="images/48-5.png" /></a><br /><br /><br />Convolutional Neural Networks<br /><br />what&#39;s really interesting about convolutions is they sound complicated but they&#39;re actually quite straightforward, <br />right? <br />It&#39;s a filter that you pass over an image in the same way as if you&#39;re doing sharpening, if you&#39;ve ever done image processing. <br />It can spot features within the image as you&#39;ve mentioned. <br />With the same paradigm of just data labels, we can let a neural network figure out for itself that it should look for shoe laces and soles or handles in bags and just learn how to detect these things by itself.<br /><br /> <br />Now, one of the things that you would have seen when you looked at the images is that there&#39;s a lot of wasted space in each image. <br />While there are only 784 pixels, it will be interesting to see if there was a way that we could condense the image down to the important features that distinguish what makes it a shoe, or a handbag, or a shirt. <br />That&#39;s where convolutions come in<br /><a href=""><img src="images/48-6.png" alt="images/48-6.png" /></a><br /><br /><br /> <br />Repeat this for each neighbor and each corresponding filter value, and would then have the new pixel with the sum of each of <br />the neighbor values multiplied by the corresponding filter value, and that&#39;s a convolution.<br />The idea here is that some convolutions will change the image in such a way that certain features in the image get emphasized. <br /><br />that&#39;s a very basic introduction to what convolutions do, and when combined with something called pooling, <br />they can become really powerful. <br />But simply, pooling is a way of compressing an image. A quick and easy way to do this, is to go over the image of four pixels at a time, i.e, the current pixel and its neighbors underneath and to the right of it. <br />Of these four, pick the biggest value and keep just that. So, for example, you can see it here. <br />My 16 pixels on the left are turned into the four pixels on the right, by looking at them in two-by-two grids and picking the biggest value. <br />This will preserve the features that were highlighted by the convolution, while simultaneously quartering the size of the image. <br />We have the horizontal and vertical axes.<br /><br /><a href=""><img src="images/48-7.png" alt="images/48-7.png" /></a><br /><a href=""><img src="images/48-8.png" alt="images/48-8.png" /></a><br /><br /><a href=""><img src="images/48-9.png" alt="images/48-9.png" /></a><br /><br /><a href=""><img src="images/48-10.png" alt="images/48-10.png" /></a><br /><br />We&#39;re asking keras to generate 64 filters for us. <br />These filters are 3 by 3, their activation is relu, which means the negative values will be thrown way, and finally the input shape is as before, the 28 by 28. That extra 1 just means that we are tallying using a single byte for color depth. As we saw before our image is our gray scale, so we just use one byte.<br />Now, of course, you might wonder what the 64 filters are. <br />It&#39;s a little beyond the scope of this class to define them, but they aren&#39;t random. <br />They start with a set of known good filters in a similar way to the pattern fitting that you saw earlier, and the ones that work from that set are learned over time.<br /><br />Implementing Pooling layers<br />This next line of code will then create a pooling layer. It&#39;s max-pooling because we&#39;re going to take the maximum value. <br />We&#39;re saying it&#39;s a two-by-two pool, so for every four pixels, the biggest one will survive as shown earlier. <br />We then add another convolutional layer, and another max-pooling layer so that the network can learn another set of convolutions on top of the existing one, and then again, pool to reduce the size. <br />So, by the time the image gets to the flatten to go into the dense layers, it&#39;s already much smaller. <br />It&#39;s being quartered, and then quartered again. So, its content has been greatly simplified, the goal being that the convolutions will filter it to the features that determine the output. <br />A really useful method on the model is the model.summary method. <br />This allows you to inspect the layers of the model, and see the journey of the image through the convolutions, and here is the output. <br />It&#39;s a nice table showing us the layers, and some details about them including the output shape.<br /><br /><a href=""><img src="images/48-11.png" alt="images/48-11.png" /></a><br /><br />it can be a little bit confusing and feel like a bug. <br />After all, isn&#39;t the data 28 by 28, so y is the output, 26 by 26. <br />The key to this is remembering that the filter is a three by three filter. <br />Consider what happens when you start scanning through an image starting on the top left. <br /><br /><a href=""><img src="images/48-12.png" alt="images/48-12.png" /></a><br /><a href=""><img src="images/48-13.png" alt="images/48-13.png" /></a><br /><br /> <br />If your filter is five-by-five for similar reasons, your output will be four smaller on x, and four smaller on y. <br />So, that&#39;s y with a three by three filter, our output from the 28 by 28 image, is now 26 by 26, we&#39;ve removed that one pixel on x and y, and each of the borders. <br />So, next is the first of the max-pooling layers. Now, remember we specified it to be two-by-two, thus turning four pixels into one, and having our x and y. <br />So, now our output gets reduced from 26 by 26, to 13 by 13. <br />The convolutions will then operate on that, and of course, we lose the one pixel margin as before, so we&#39;re down to 11 by 11, add another two-by-two max-pooling to have this rounding down, and went down, down to five-by-five images. <br />So, now our dense neural network is the same as before, but it&#39;s being fed with five-by-five images instead of 28 by 28 ones. <br />But remember, it&#39;s not just one compress five-by-five image instead of the original 28 by 28, there are a number of convolutions per image that we specified, in this case 64. So, there are 64 new images of five-by-five that had been fed in. <br />Flatten that out and you have 25 pixels times 64, which is 1600. So, you can see that the new flattened layer has 1,600 elements in it, as opposed to the 784 that you had previously. This number is impacted by the parameters that you set when defining the convolutional 2D layers. <br />Later when you experiment, you&#39;ll see what the impact of setting what other values for the number of convolutions will be, and in particular, you can see what happens when you&#39;re feeding less than 784 over all pixels in. <br />Training should be faster, but is there a sweet spot where it&#39;s more accurate? <br /><br /></div></body></html>