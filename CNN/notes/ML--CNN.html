<!doctype html><html>
    <head>
      <meta http-equiv="content-type" content="text/html; charset=utf-8">
      <title>CNN</title>
      <meta name="generator" content="CherryTree">
      <link rel="stylesheet" href="res/styles3.css" type="text/css" />
    </head>
    <body><div class='page'><h1 class='title'>CNN</h1><br/>Training large datasets<br /><br />Augmenting the image <br />Some of the nice things with being able to do image augmentation is that we can then, <br />I think you just use the term create new data, which is effectively what we&#39;re doing. <br />So for example, if we have a cat and our cats in our training dataset are always upright and their ears are like this, we may not spot a cat that&#39;s lying down. But with augmentation, being able to rotate the image, or being able to skew the image, or maybe some other transforms would be able to effectively generate that data to train off. <br />So you skew the image and just toss that into the training set. <br />But there&#39;s an important trick to how you do this in TensorFlow as well to not take an image, warp it, skew it, and then blow up the memory requirements. So TensorFlow makes it really easy to do this.<br /><br /><br />We use ImageDataGenerator for this <br /><a href=""><img src="images/49-1.png" alt="images/49-1.png" /></a><br />Rotation range is a range from 0-180 degrees with which to randomly rotate images. <br />So in this case, the image will rotate by random amount between 0 and 40 degrees. Shifting, moves the image around inside its frame. <br />Many pictures have the subject centered. So if we train based on those kind of images, we might over-fit for that scenario. These parameters specify, as a proportion of the image size, how much we should randomly move the subject around. <br />So in this case, we might offset it by 20 percent vertically or horizontally. Shearing is also quite powerful. So for example, consider the image on the right. We know that it&#39;s a person. But in our training set, we don&#39;t have any images of a person in that orientation. However, we do have an image like this one, where the person is oriented similarly. <br />So if we shear that person by skewing along the x-axis, we&#39;ll end up in a similar pose. <br />That&#39;s what the shear_range parameter gives us. It will shear the image by random amounts up to the specified portion in the image. <br />So in this case, it will shear up to 20 percent of the image. Zoom can also be very effective. <br />For example, consider the image on the right. It&#39;s obviously a woman facing to the right. <br />Our image on the left is from the humans or horses training set. It&#39;s very similar but it zoomed out to see the full person. <br />If we zoom in on the training image, we could end up with a very similar image to the one on the right. <br />Thus, if we zoom while training, we could spot more generalized examples like this one. So you zoom with code like this. The 0.2 is a relative portion of the image you will zoom in on. <br />So in this case, zooms will be a random amount up to 20 percent of the size of the image. <br />Another useful tool is horizontal flipping. So for example, if you consider the picture on the right, we might not be able to classify it correctly as our training data doesn&#39;t have the image of a woman with her left hand raised, it does have the image on the left, where the subjects right arm is raised. So if the image were flipped horizontally, then it becomes more structurally similar to the image on the right and we might not over-fit to right arm raisers. To turn on random horizontal flipping, you just say horizontal_flip equals true and the images will be flipped at random. <br />Finally, we just specify the fill mode. <br />This fills in any pixels that might have been lost by the operations. I&#39;m just going to stick with nearest here, which uses neighbors of that pixel to try and keep uniformity. Check the carets documentation for some other options. So that&#39;s the concept of image augmentation.<br /><a href=""><img src="images/49-2.png" alt="images/49-2.png" /></a><br /><a href=""><img src="images/49-3.png" alt="images/49-3.png" /></a><br /><a href=""><img src="images/49-4.png" alt="images/49-4.png" /></a><br /><a href=""><img src="images/49-5.png" alt="images/49-5.png" /></a><br /><a href=""><img src="images/49-6.png" alt="images/49-6.png" /></a><a href=""><img src="images/49-7.png" alt="images/49-7.png" /></a><br /><br /><br />Transfer Learning<br /><br />Rather than needing to train a neural network from scratch we can need a lot of data and take a long time to train, you can instead download maybe an open-source model that someone else has already trained on a huge dataset maybe for weeks and use those parameters as a starting point to then train your model just a little bit more on perhaps a smaller dataset that you have for a given task, so it is called transfer learning. <br /><br /><a href=""><img src="images/49-8.png" alt="images/49-8.png" /></a><br /><a href=""><img src="images/49-9.png" alt="images/49-9.png" /></a><br /><br />Inception<br />Imagenet<br /><br /><a href=""><img src="images/49-10.png" alt="images/49-10.png" /></a><a href=""><img src="images/49-11.png" alt="images/49-11.png" /></a><br />snapshot of model after trained<br /><a href=""><img src="images/49-12.png" alt="images/49-12.png" /></a><br />it&#39;s fortunate that keras has the model definition built in. <br />So you instantiate that with the desired input shape for your data, and specify that you don&#39;t want to use the built-in weights, but the snapshot that you&#39;ve just downloaded. The inception V3 has a fully-connected layer at the top. <br />So by setting include_top to false, you&#39;re specifying that you want to ignore this and get straight to the convolutions. <br />Now that I have my pretrained model instantiated, I can iterate through its layers and lock them, saying that they&#39;re not going to be trainable with this code. You can then print a summary of your pretrained model with this code but be prepared, it&#39;s huge. There&#39;s no way I can fit it all in a slide, even if I use a two point font like this. <br /><br /><a href=""><img src="images/49-13.png" alt="images/49-13.png" /></a><br /><a href=""><img src="images/49-14.png" alt="images/49-14.png" /></a><br />But with this code, I&#39;m going to grab that layer from inception and <br />take it to output.<br /><a href=""><img src="images/49-15.png" alt="images/49-15.png" /></a><br /><br /><a href=""><img src="images/49-16.png" alt="images/49-16.png" /></a><br /><br />dropouts<br /><a href=""><img src="images/49-17.png" alt="images/49-17.png" /></a><br />- randomly knocking out units in the network<br />- working on smaller neural network<br />- main unit can rely on one feature. cant put weight on one previous units <br />- different keep_prob explain a range for avoid overrfitting in those specific layers<br />- lower the keep_prob makes major dropout functionality (like updating a specific lambda value on the polynomial function)<br />- downside it gives us more hyperparams using cross validation<br />- computer vision uses alot dropout - a regularization technique for overfitting <br />- J cost function is not well defined anymore - losses it as debugging tool for double check <br />- <a href=""><img src="images/49-18.png" alt="images/49-18.png" /></a> turn off dropout and check it is monotonically decreasing J and check the code is working<br /><br />exploring dropout <br /><a href=""><img src="images/49-19.png" alt="images/49-19.png" /></a><a href=""><img src="images/49-20.png" alt="images/49-20.png" /></a><br />By dropping some out, we make it look like this. <br />And that has the effect of neighbors not affecting each other too much and potentially removing overfitting.<br /><br /><a href=""><img src="images/49-21.png" alt="images/49-21.png" /></a><a href=""><img src="images/49-22.png" alt="images/49-22.png" /></a><br /><br />Multiclass classification<br /><a href=""><img src="images/49-23.png" alt="images/49-23.png" /></a><br /><a href="https://laurencemoroney.com/datasets.html">https://laurencemoroney.com/datasets.html</a><br /><br /><a href=""><img src="images/49-24.png" alt="images/49-24.png" /></a><a href=""><img src="images/49-25.png" alt="images/49-25.png" /></a><br /><a href=""><img src="images/49-26.png" alt="images/49-26.png" /></a><a href=""><img src="images/49-27.png" alt="images/49-27.png" /></a><br /><a href=""><img src="images/49-28.png" alt="images/49-28.png" /></a><br /><a href=""><img src="images/49-29.png" alt="images/49-29.png" /></a><a href=""><img src="images/49-30.png" alt="images/49-30.png" /></a><br /><a href=""><img src="images/49-31.png" alt="images/49-31.png" /></a><br /><br /><br /></div></body></html>