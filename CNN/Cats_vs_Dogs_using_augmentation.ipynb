{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dn-6c02VmqiN"
   },
   "outputs": [],
   "source": [
    "# ATTENTION: Please do not alter any of the provided code in the exercise. Only add your own code where indicated\n",
    "# ATTENTION: Please do not add or remove any cells in the exercise. The grader will check specific cells based on the cell position.\n",
    "# ATTENTION: Please use the provided epoch values when training.\n",
    "\n",
    "# In this exercise you will train a CNN on the FULL Cats-v-dogs dataset\n",
    "# This will require you doing a lot of data preprocessing because\n",
    "# the dataset isn't split into training and validation for you\n",
    "# This code block has all the required inputs\n",
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from shutil import copyfile\n",
    "from os import getcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3sd9dQWa23aj"
   },
   "outputs": [],
   "source": [
    "# This code block unzips the full Cats-v-Dogs dataset to /tmp\n",
    "# which will create a tmp/PetImages directory containing subdirectories\n",
    "# called 'Cat' and 'Dog' (that's how the original researchers structured it)\n",
    "path_cats_and_dogs = f\"{getcwd()}/../tmp2/cats-and-dogs.zip\"\n",
    "shutil.rmtree('/tmp')\n",
    "\n",
    "local_zip = path_cats_and_dogs\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gi3yD62a6X3S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir('/tmp/PetImages/Cat/')))\n",
    "print(len(os.listdir('/tmp/PetImages/Dog/')))\n",
    "\n",
    "# Expected Output:\n",
    "# 1500\n",
    "# 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-QkLjxpmyK2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created\n"
     ]
    }
   ],
   "source": [
    "# Use os.mkdir to create your directories\n",
    "# You will need a directory for cats-v-dogs, and subdirectories for training\n",
    "# and testing. These in turn will need subdirectories for 'cats' and 'dogs'\n",
    "try:\n",
    "    os.mkdir('/tmp/cats-v-dogs')\n",
    "    os.mkdir('/tmp/cats-v-dogs/training')\n",
    "    os.mkdir('/tmp/cats-v-dogs/testing')\n",
    "    os.mkdir('/tmp/cats-v-dogs/training/dogs')\n",
    "    os.mkdir('/tmp/cats-v-dogs/training/cats')\n",
    "    os.mkdir('/tmp/cats-v-dogs/testing/dogs')\n",
    "    os.mkdir('/tmp/cats-v-dogs/testing/cats')\n",
    "    print('created')\n",
    "except :\n",
    "    print('failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zvSODo0f9LaU"
   },
   "outputs": [],
   "source": [
    "# Write a python function called split_data which takes\n",
    "# a SOURCE directory containing the files\n",
    "# a TRAINING directory that a portion of the files will be copied to\n",
    "# a TESTING directory that a portion of the files will be copie to\n",
    "# a SPLIT SIZE to determine the portion\n",
    "# The files should also be randomized, so that the training set is a random\n",
    "# X% of the files, and the test set is the remaining files\n",
    "# SO, for example, if SOURCE is PetImages/Cat, and SPLIT SIZE is .9\n",
    "# Then 90% of the images in PetImages/Cat will be copied to the TRAINING dir\n",
    "# and 10% of the images will be copied to the TESTING dir\n",
    "# Also -- All images should be checked, and if they have a zero file length,\n",
    "# they will not be copied over\n",
    "#\n",
    "# os.listdir(DIRECTORY) gives you a listing of the contents of that directory\n",
    "# os.path.getsize(PATH) gives you the size of the file\n",
    "# copyfile(source, destination) copies a file from source to destination\n",
    "# random.sample(list, len(list)) shuffles a list\n",
    "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
    "# YOUR CODE STARTS HERE\n",
    "    all_files=[]\n",
    "    for file_name in os.listdir(SOURCE):\n",
    "        file_path = SOURCE + file_name\n",
    "    \n",
    "        if os.path.getsize(file_path):\n",
    "            all_files.append(file_name)\n",
    "        else:\n",
    "            print('{} is zero length, so ignoring'.format(file_name))\n",
    "        \n",
    "    n_files=len(all_files)\n",
    "    split_point= int(n_files * SPLIT_SIZE)\n",
    "    \n",
    "    shuffled = random.sample(all_files, n_files)\n",
    "    \n",
    "    train_set = shuffled[:split_point]\n",
    "    test_set = shuffled[split_point:]\n",
    "    \n",
    "    for file_name in train_set:\n",
    "        copyfile(SOURCE + file_name, TRAINING + file_name)\n",
    "        \n",
    "    for file_name in test_set:\n",
    "        copyfile(SOURCE + file_name, TESTING + file_name)           \n",
    "# YOUR CODE ENDS HERE\n",
    "# YOUR CODE ENDS HERE\n",
    "\n",
    "\n",
    "CAT_SOURCE_DIR = \"/tmp/PetImages/Cat/\"\n",
    "TRAINING_CATS_DIR = \"/tmp/cats-v-dogs/training/cats/\"\n",
    "TESTING_CATS_DIR = \"/tmp/cats-v-dogs/testing/cats/\"\n",
    "DOG_SOURCE_DIR = \"/tmp/PetImages/Dog/\"\n",
    "TRAINING_DOGS_DIR = \"/tmp/cats-v-dogs/training/dogs/\"\n",
    "TESTING_DOGS_DIR = \"/tmp/cats-v-dogs/testing/dogs/\"\n",
    "\n",
    "split_size = .9\n",
    "split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\n",
    "split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "luthalB76ufC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350\n",
      "1350\n",
      "150\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir('/tmp/cats-v-dogs/training/cats/')))\n",
    "print(len(os.listdir('/tmp/cats-v-dogs/training/dogs/')))\n",
    "print(len(os.listdir('/tmp/cats-v-dogs/testing/cats/')))\n",
    "print(len(os.listdir('/tmp/cats-v-dogs/testing/dogs/')))\n",
    "\n",
    "# Expected output:\n",
    "# 1350\n",
    "# 1350\n",
    "# 150\n",
    "# 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-BQrav4anTmj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               9470464   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 9,499,617\n",
      "Trainable params: 9,499,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# DEFINE A KERAS MODEL TO CLASSIFY CATS V DOGS\n",
    "# USE AT LEAST 3 CONVOLUTION LAYERS\n",
    "model = tf.keras.models.Sequential([\n",
    "# YOUR CODE HERE\n",
    "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2), \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'), \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(), \n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'), \n",
    "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('cats') and 1 for the other ('dogs')\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=RMSprop(learning_rate=0.001), loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE:\n",
    "\n",
    "In the cell below you **MUST** use a batch size of 10 (`batch_size=10`) for the `train_generator` and the `validation_generator`. Using a batch size greater than 10 will exceed memory limits on the Coursera platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mlNjoJ5D61N6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2700 images belonging to 2 classes.\n",
      "Found 300 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = '/tmp/cats-v-dogs/training'#YOUR CODE HERE\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1 / 255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.4,\n",
    "    height_shift_range=0.4,\n",
    "    shear_range=0.4,\n",
    "    zoom_range=0.4,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "#YOUR CODE HERE\n",
    "\n",
    "# NOTE: YOU MUST USE A BATCH SIZE OF 10 (batch_size=10) FOR THE \n",
    "# TRAIN GENERATOR.\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAINING_DIR,\n",
    "    batch_size=10,\n",
    "    class_mode='binary',\n",
    "    target_size=(150, 150)\n",
    ") #YOUR CODE HERE\n",
    "\n",
    "VALIDATION_DIR = '/tmp/cats-v-dogs/testing' #YOUR CODE HERE\n",
    "validation_datagen = ImageDataGenerator(rescale= 1/255)\n",
    "#YOUR CODE HERE\n",
    "\n",
    "# NOTE: YOU MUST USE A BACTH SIZE OF 10 (batch_size=10) FOR THE \n",
    "# VALIDATION GENERATOR.\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    VALIDATION_DIR,\n",
    "    batch_size=10,\n",
    "    class_mode='binary',\n",
    "    target_size=(150, 150)\n",
    ")#YOUR CODE HERE\n",
    "\n",
    "# Expected Output:\n",
    "# Found 2700 images belonging to 2 classes.\n",
    "# Found 300 images belonging to 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KyS4n53w7DxC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "270/270 [==============================] - 57s 210ms/step - loss: 0.6988 - acc: 0.5974 - val_loss: 0.6272 - val_acc: 0.6533\n",
      "Epoch 2/3\n",
      "  1/270 [..............................] - ETA: 30s - loss: 0.6485 - acc: 0.5000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e96c8ad2c0df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                               validation_data=validation_generator)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    250\u001b[0m               \u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m               \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m               training=training))\n\u001b[0m\u001b[1;32m    253\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         raise ValueError('The model cannot be run '\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_model_loss\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    254\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    706\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    707\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=3,\n",
    "                              verbose=1,\n",
    "                              validation_data=validation_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWZrJN4-65RC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training and validation loss')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLOT LOSS AND ACCURACY\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "acc=history.history['acc']\n",
    "val_acc=history.history['val_acc']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.figure()\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
    "\n",
    "\n",
    "plt.title('Training and validation loss')\n",
    "\n",
    "# Desired output. Charts with training and validation metrics. No crash :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now click the 'Submit Assignment' button above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When you're done or would like to take a break, please run the two cells below to save your work and close the Notebook. This will free up resources for your fellow learners. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "<!-- Save the notebook -->\n",
    "IPython.notebook.save_checkpoint();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.notebook.session.delete();\n",
    "window.onbeforeunload = null\n",
    "setTimeout(function() { window.close(); }, 1000);"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise 6 - Question.ipynb",
   "provenance": []
  },
  "coursera": {
   "course_slug": "convolutional-neural-networks-tensorflow",
   "graded_item_id": "uAPOR",
   "launcher_item_id": "e9lTb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
