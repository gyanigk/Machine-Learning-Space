{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport re\nimport csv\nimport time\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-31T19:33:13.024768Z","iopub.execute_input":"2021-07-31T19:33:13.025176Z","iopub.status.idle":"2021-07-31T19:33:13.037168Z","shell.execute_reply.started":"2021-07-31T19:33:13.025095Z","shell.execute_reply":"2021-07-31T19:33:13.036252Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/amazon-ml-challenge-2021-hackerearth/train.csv\", escapechar=\"\\\\\", quoting=csv.QUOTE_NONE, usecols=[\"TITLE\", \"BROWSE_NODE_ID\"])\ntest = pd.read_csv(\"../input/amazon-ml-challenge-2021-hackerearth/test.csv\", escapechar=\"\\\\\", quoting=csv.QUOTE_NONE, usecols=[\"PRODUCT_ID\", \"TITLE\"])\nss = pd.read_csv(\"../input/amazon-ml-challenge-2021-hackerearth/sample_submission.csv\", escapechar=\"\\\\\", quoting=csv.QUOTE_NONE)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T19:33:15.589921Z","iopub.execute_input":"2021-07-31T19:33:15.590346Z","iopub.status.idle":"2021-07-31T19:34:02.761862Z","shell.execute_reply.started":"2021-07-31T19:33:15.590295Z","shell.execute_reply":"2021-07-31T19:34:02.760783Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# temp fix of nan values\ntrain = train.fillna(\" \")","metadata":{"execution":{"iopub.status.busy":"2021-07-31T19:35:22.817082Z","iopub.execute_input":"2021-07-31T19:35:22.817475Z","iopub.status.idle":"2021-07-31T19:35:23.157537Z","shell.execute_reply.started":"2021-07-31T19:35:22.817442Z","shell.execute_reply":"2021-07-31T19:35:23.156673Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# TITLE Only","metadata":{}},{"cell_type":"code","source":"def clean_title(string):\n    # remove special characters\n    string = re.sub(\"[^a-zA-Z0-9]\\s?\", ' ', string)\n    # remove single characters\n    string = re.sub(\"(^| ).(( ).)*( |$)\", ' ', string)\n    # remove repeated spaces\n    string = re.sub(r\"\\s+\", \" \", string)\n    # lower all characters\n    string = string.lower()\n    # remove html/css stuffs\n    return string\n    pass","metadata":{"execution":{"iopub.status.busy":"2021-07-31T19:35:48.546732Z","iopub.execute_input":"2021-07-31T19:35:48.547083Z","iopub.status.idle":"2021-07-31T19:35:48.552673Z","shell.execute_reply.started":"2021-07-31T19:35:48.547047Z","shell.execute_reply":"2021-07-31T19:35:48.551585Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train[\"cleaned_title\"] = train[\"TITLE\"].progress_apply(clean_title)\ndel train[\"TITLE\"]\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T19:35:49.719360Z","iopub.execute_input":"2021-07-31T19:35:49.719686Z","iopub.status.idle":"2021-07-31T19:37:20.212792Z","shell.execute_reply.started":"2021-07-31T19:35:49.719658Z","shell.execute_reply":"2021-07-31T19:37:20.211968Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"100%|██████████| 2903024/2903024 [01:30<00:00, 32152.19it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, TFBertModel, create_optimizer\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Flatten, Input\nfrom keras.preprocessing.sequence import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2021-07-31T19:37:22.123620Z","iopub.execute_input":"2021-07-31T19:37:22.123952Z","iopub.status.idle":"2021-07-31T19:37:28.332921Z","shell.execute_reply.started":"2021-07-31T19:37:22.123919Z","shell.execute_reply":"2021-07-31T19:37:28.332097Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    MAX_LEN_TITLE = 96\n    EPOCHS = 20\n    TRAIN_BS = 32\n    VALIDATION_BS = 64\n    N_CLASSES = 9919\n    \n    model_name = \"bert-base-uncased\"\n    tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)\n    \ndef tokenize_sentences(sentences, tokenizer, max_seq_len):\n    tokenized_sentences = []\n\n    for sentence in tqdm(sentences):\n        tokenized_sentence = tokenizer.encode(\n                            sentence,                  # Sentence to encode.\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            max_length = max_seq_len,  # Truncate all sentences.\n                            truncation=True\n                    )\n        \n        tokenized_sentences.append(tokenized_sentence)\n\n    return tokenized_sentences\n\ndef create_attention_masks(tokenized_and_padded_sentences):\n    attention_masks = []\n\n    for sentence in tokenized_and_padded_sentences:\n        att_mask = [int(token_id > 0) for token_id in sentence]\n        attention_masks.append(att_mask)\n\n    return np.asarray(attention_masks)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T19:37:28.334467Z","iopub.execute_input":"2021-07-31T19:37:28.334797Z","iopub.status.idle":"2021-07-31T19:37:32.382701Z","shell.execute_reply.started":"2021-07-31T19:37:28.334762Z","shell.execute_reply":"2021-07-31T19:37:32.381951Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f20be28acf4d47cfbe0f02acf15a63e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54c5f7d388914e7da385003a68e8051d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72fc1fa54ef3442facd7dd90a1635827"}},"metadata":{}}]},{"cell_type":"code","source":"def focal_loss(inputs, targets, alpha=1, gamma=2):\n    ce_loss = tf.keras.losses.categorical_crossentropy(inputs, targets)\n    pt = K.exp(-ce_loss)\n    _focal_loss = alpha * (1 - pt)**gamma * ce_loss\n\n    return _focal_loss\n    pass","metadata":{"execution":{"iopub.status.busy":"2021-07-31T19:37:32.384315Z","iopub.execute_input":"2021-07-31T19:37:32.384841Z","iopub.status.idle":"2021-07-31T19:37:32.391348Z","shell.execute_reply.started":"2021-07-31T19:37:32.384801Z","shell.execute_reply":"2021-07-31T19:37:32.390225Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"input_ids = tokenize_sentences(train['cleaned_title'], CFG.tokenizer, CFG.MAX_LEN_TITLE)\ninput_ids = pad_sequences(input_ids, maxlen=CFG.MAX_LEN_TITLE, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\nattention_masks = create_attention_masks(input_ids)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T19:37:34.973155Z","iopub.execute_input":"2021-07-31T19:37:34.973472Z","iopub.status.idle":"2021-07-31T20:12:26.680113Z","shell.execute_reply.started":"2021-07-31T19:37:34.973446Z","shell.execute_reply":"2021-07-31T20:12:26.679188Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"100%|██████████| 2903024/2903024 [28:45<00:00, 1682.91it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"le = LabelEncoder().fit(train[\"BROWSE_NODE_ID\"].values)\nlabels_map = le.transform(train[\"BROWSE_NODE_ID\"].values)\nlabels = tf.keras.utils.to_categorical(labels_map)\n\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=0, test_size=0.3)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=0, test_size=0.3)\n\ntrain_size = len(train_inputs)\nvalidation_size = len(validation_inputs)\n\n# train_dataset = create_dataset(((train_inputs, train_masks), train_labels), batch_size=CFG.TRAIN_BS)\n# validation_dataset = create_dataset(((validation_inputs, validation_masks), validation_labels), batch_size=CFG.VALIDATION_BS)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T20:12:46.318308Z","iopub.execute_input":"2021-07-31T20:12:46.318646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(max_len, n_classes, loss, optimizer):\n    ids = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n    att = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n\n    bert_model = TFBertModel.from_pretrained(CFG.model_name)\n    x = bert_model(ids,attention_mask=att)\n    out = Dense(n_classes, activation=\"softmax\")(x[1])\n\n    model = tf.keras.models.Model(inputs=[ids, att], outputs=out)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n    model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n    \n    return model\n\nmodel = build_model(CFG.MAX_LEN_TITLE, len(le.classes_),\n                    loss=focal_loss,\n                    optimizer=\"adam\")\n\nhistory = model.fit(x=(train_inputs, train_masks),\n                    y=train_labels,\n                    validation_data=((validation_inputs, validation_masks), validation_labels),\n                    steps_per_epoch=train_size//CFG.TRAIN_BS,\n                    validation_steps=validation_size//CFG.VALIDATION_BS,\n                    epochs=10)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T19:29:39.547773Z","iopub.execute_input":"2021-07-31T19:29:39.548163Z","iopub.status.idle":"2021-07-31T19:31:28.176075Z","shell.execute_reply.started":"2021-07-31T19:29:39.548132Z","shell.execute_reply":"2021-07-31T19:31:28.175268Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n21/21 [==============================] - 25s 509ms/step - loss: 6.5124 - accuracy: 0.0285 - val_loss: 7.8241 - val_accuracy: 0.0000e+00\nEpoch 2/10\n21/21 [==============================] - 9s 429ms/step - loss: 6.4377 - accuracy: 0.0274 - val_loss: 7.1135 - val_accuracy: 0.0533\nEpoch 3/10\n21/21 [==============================] - 9s 428ms/step - loss: 6.0708 - accuracy: 0.0572 - val_loss: 7.3724 - val_accuracy: 0.0533\nEpoch 4/10\n21/21 [==============================] - 9s 428ms/step - loss: 6.0137 - accuracy: 0.0465 - val_loss: 7.5648 - val_accuracy: 0.0533\nEpoch 5/10\n21/21 [==============================] - 9s 431ms/step - loss: 6.0418 - accuracy: 0.0357 - val_loss: 7.6489 - val_accuracy: 0.0533\nEpoch 6/10\n21/21 [==============================] - 9s 429ms/step - loss: 5.9966 - accuracy: 0.0510 - val_loss: 7.9384 - val_accuracy: 0.0533\nEpoch 7/10\n21/21 [==============================] - 9s 430ms/step - loss: 5.9831 - accuracy: 0.0666 - val_loss: 7.6994 - val_accuracy: 0.0533\nEpoch 8/10\n21/21 [==============================] - 9s 432ms/step - loss: 6.0114 - accuracy: 0.0541 - val_loss: 7.9439 - val_accuracy: 0.0533\nEpoch 9/10\n21/21 [==============================] - 9s 429ms/step - loss: 5.9712 - accuracy: 0.0519 - val_loss: 8.0384 - val_accuracy: 0.0533\nEpoch 10/10\n21/21 [==============================] - 9s 428ms/step - loss: 5.9929 - accuracy: 0.0460 - val_loss: 8.0044 - val_accuracy: 0.0533\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"# fill NaN\ntest = test.fillna(\" \")\n\ntest[\"cleaned_title\"] = test[\"TITLE\"].progress_apply(clean_title)\ntest_input_ids = tokenize_sentences(test['cleaned_title'], CFG.tokenizer, CFG.MAX_LEN_TITLE)\ntest_input_ids = pad_sequences(test_input_ids, maxlen=CFG.MAX_LEN_TITLE, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\ntest_attention_masks = create_attention_masks(test_input_ids)\n\npreds = model.predict((test_input_ids, test_attention_masks),\n                     batch_size=64, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T19:08:44.915671Z","iopub.execute_input":"2021-07-31T19:08:44.915991Z","iopub.status.idle":"2021-07-31T19:16:52.349159Z","shell.execute_reply.started":"2021-07-31T19:08:44.915960Z","shell.execute_reply":"2021-07-31T19:16:52.348286Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"100%|██████████| 110775/110775 [00:03<00:00, 30174.48it/s]\n100%|██████████| 110775/110775 [01:12<00:00, 1518.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"1731/1731 [==============================] - 397s 229ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"sub = pd.DataFrame()\nsub[\"PRODUCT_ID\"] = test[\"PRODUCT_ID\"].values\nsub[\"BROWSE_NODE_ID\"] = le.inverse_transform(np.argmax(preds, axis=1))\n\nsub","metadata":{"execution":{"iopub.status.busy":"2021-07-31T19:19:23.028499Z","iopub.execute_input":"2021-07-31T19:19:23.028819Z","iopub.status.idle":"2021-07-31T19:19:23.155335Z","shell.execute_reply.started":"2021-07-31T19:19:23.028789Z","shell.execute_reply":"2021-07-31T19:19:23.154343Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"        PRODUCT_ID  BROWSE_NODE_ID\n0                1               4\n1                2              55\n2                3              55\n3                4              98\n4                5              55\n...            ...             ...\n110770      110771              55\n110771      110772             209\n110772      110773              55\n110773      110774              75\n110774      110775             209\n\n[110775 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PRODUCT_ID</th>\n      <th>BROWSE_NODE_ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>98</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>110770</th>\n      <td>110771</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>110771</th>\n      <td>110772</td>\n      <td>209</td>\n    </tr>\n    <tr>\n      <th>110772</th>\n      <td>110773</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>110773</th>\n      <td>110774</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>110774</th>\n      <td>110775</td>\n      <td>209</td>\n    </tr>\n  </tbody>\n</table>\n<p>110775 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T19:19:25.856787Z","iopub.execute_input":"2021-07-31T19:19:25.857124Z","iopub.status.idle":"2021-07-31T19:19:26.037674Z","shell.execute_reply.started":"2021-07-31T19:19:25.857093Z","shell.execute_reply":"2021-07-31T19:19:26.036733Z"},"trusted":true},"execution_count":15,"outputs":[]}]}