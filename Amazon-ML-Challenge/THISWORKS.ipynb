{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1627726522924
    },
    "id": "LyLJk3IV7e39",
    "outputId": "eb6b0837-7c6c-45a7-9241-6533b16bc8ea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping keras as it is not installed.\n",
      "WARNING: Skipping keras-nightly as it is not installed.\n",
      "WARNING: Skipping keras-Preprocessing as it is not installed.\n",
      "WARNING: Skipping keras-vis as it is not installed.\n",
      "WARNING: Skipping tensorflow as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.5.0-cp37-cp37m-win_amd64.whl (422.6 MB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting tensorboard~=2.5\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp37-cp37m-win_amd64.whl (2.7 MB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp37-cp37m-win_amd64.whl (13.2 MB)\n",
      "Collecting wheel~=0.35\n",
      "  Using cached wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.17.3-cp37-cp37m-win_amd64.whl (909 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "Collecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp37-cp37m-win_amd64.whl (2.9 MB)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting six~=1.15.0\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "Collecting cached-property\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting setuptools>=41.0.0\n",
      "  Downloading setuptools-57.4.0-py3-none-any.whl (819 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.34.0-py2.py3-none-any.whl (152 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (0.14.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (2.21.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.5-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (0.0.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.24.1)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Requirement already satisfied: zipp>=0.3.2 in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (0.3.3)\n",
      "Building wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4834 sha256=699620586322f7197477cc09a4a8c90e1960760d9249100075db1f277ed13aaa\n",
      "  Stored in directory: c:\\users\\gyani\\appdata\\local\\pip\\cache\\wheels\\3f\\e3\\ec\\8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-win_amd64.whl size=33366 sha256=1c4f99432b861e0c22d32896947fdb17b7951fb86653a16ebef97562d8aeba4b\n",
      "  Stored in directory: c:\\users\\gyani\\appdata\\local\\pip\\cache\\wheels\\62\\76\\4c\\aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: six, setuptools, rsa, oauthlib, cachetools, requests-oauthlib, google-auth, wheel, tensorboard-plugin-wit, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, cached-property, absl-py, wrapt, typing-extensions, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras-nightly, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.12.0\n",
      "    Uninstalling six-1.12.0:\n",
      "      Successfully uninstalled six-1.12.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 40.8.0\n",
      "    Uninstalling setuptools-40.8.0:\n",
      "      Successfully uninstalled setuptools-40.8.0\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.33.1\n",
      "    Uninstalling wheel-0.33.1:\n",
      "      Successfully uninstalled wheel-0.33.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.16.2\n",
      "    Uninstalling numpy-1.16.2:\n",
      "      Successfully uninstalled numpy-1.16.2\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.11.1\n",
      "    Uninstalling wrapt-1.11.1:\n",
      "      Successfully uninstalled wrapt-1.11.1\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.9.0\n",
      "    Uninstalling h5py-2.9.0:\n",
      "      Successfully uninstalled h5py-2.9.0\n",
      "Successfully installed absl-py-0.13.0 astunparse-1.6.3 cached-property-1.5.2 cachetools-4.2.2 flatbuffers-1.12 gast-0.4.0 google-auth-1.34.0 google-auth-oauthlib-0.4.5 google-pasta-0.2.0 grpcio-1.34.1 h5py-3.1.0 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.17.3 requests-oauthlib-1.3.0 rsa-4.7.2 setuptools-57.4.0 six-1.15.0 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.5.0 tensorflow-estimator-2.5.0 termcolor-1.1.0 typing-extensions-3.7.4.3 wheel-0.36.2 wrapt-1.12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.5.1 requires mock>=2.0, which is not installed.\n",
      "astroid 2.2.5 requires typed-ast>=1.3.0; implementation_name == \"cpython\", which is not installed.\n",
      "glue-core 0.15.6 requires ipykernel!=5.0.0,!=5.1.0,>=4.0, but you have ipykernel 5.1.0 which is incompatible.\n",
      "automat 20.2.0 requires attrs>=19.2.0, but you have attrs 19.1.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from keras) (1.19.5)\n",
      "Requirement already satisfied: h5py in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from keras) (3.1.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from keras) (1.2.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from keras) (5.1)\n",
      "Requirement already satisfied: cached-property in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from h5py->keras) (1.5.2)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.4.3\n",
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.8.1-cp37-cp37m-win_amd64.whl (154 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from wordcloud) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from wordcloud) (1.19.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from wordcloud) (3.0.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.0)\n",
      "Requirement already satisfied: six in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gyani\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (57.4.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.8.1\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall keras -y\n",
    "# !pip uninstall keras-nightly -y\n",
    "# !pip uninstall keras-Preprocessing -y\n",
    "# !pip uninstall keras-vis -y\n",
    "# !pip uninstall tensorflow -y\n",
    "# !pip install tensorflow\n",
    "# !pip install keras \n",
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1627751705869
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam#Try all of these\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1627744689030
    },
    "id": "MfQJxQLb76jp",
    "outputId": "bb45954f-0ee7-44be-b639-c25dea1cf187"
   },
   "outputs": [],
   "source": [
    "# !wget \"https://s3-ap-southeast-1.amazonaws.com/he-public-data/dataset52a7b21.zip\"\n",
    "# !unzip dataset52a7b21.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\gyani\\\\Desktop'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1627744762097
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2903024"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1627744869799
    },
    "id": "64S_1Iyw-FQu"
   },
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('dataset/train.csv',escapechar = \"\\\\\",quoting = csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1627744870166
    },
    "id": "GpGbsCIc8SgA"
   },
   "outputs": [],
   "source": [
    "x = train_df['TITLE'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1627744870743
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "GPU device not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0cc9dc201ab5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'/device:GPU:0'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m  \u001b[1;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU device not found'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Found GPU at: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: GPU device not found"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    " raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "gather": {
     "logged": 1627744973302
    },
    "id": "UzccOX546Mcs"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y=le.fit_transform(train_df['BROWSE_NODE_ID'])\n",
    "#y=np.expand_dims(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "gather": {
     "logged": 1627744975079
    },
    "id": "IUjbeiAq6MCz"
   },
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "gather": {
     "logged": 1627744978261
    },
    "id": "SWhnsWID-8LT"
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "max_text_length = 400 #find the text with max length in code and change it for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1627745015146
    },
    "id": "R7SYc_Mp_aDn",
    "outputId": "0b6ea1bc-c676-4448-c5cf-08238e932be5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happened\n",
      "Happened\n"
     ]
    }
   ],
   "source": [
    "x_tokenizer =text.Tokenizer(max_features)\n",
    "x_tokenizer.fit_on_texts(list(x))\n",
    "print(\"Happened\")\n",
    "x_tokenized = x_tokenizer.texts_to_sequences(x)\n",
    "print(\"Happened\")\n",
    "x_train_val = sequence.pad_sequences(x_tokenized, maxlen=max_text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1627745015444
    },
    "id": "INYtPwh8_dE9",
    "outputId": "8f810085-a4cc-4c05-d503-d9d1b28ea119"
   },
   "outputs": [],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1627745060650
    },
    "id": "zj08cHToVSA1",
    "outputId": "c55d4662-7765-4f60-ebce-dbb29b1a6326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "embedding_index = dict()\n",
    "f = open('glove.6B.300d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embedding_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embedding_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "gather": {
     "logged": 1627745060794
    },
    "id": "hLAOP9ozYIx1"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "for word, index in x_tokenizer.word_index.items():\n",
    "  if index>max_features -1:\n",
    "    break\n",
    "  else:\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      embedding_matrix[index] = embedding_vector\n",
    "del(embedding_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627748965518
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "embedding_matrix=np.float32(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627745061096
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "dbfile = open('embeddingmatrix.pkl','ab')\n",
    "pickle.dump(embedding_matrix, dbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627748968733
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "max_features = 20000\n",
    "max_text_length = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627748979258
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val,\n",
    "                                                  y,\n",
    "                                                  random_state=1,\n",
    "                                                  test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "gather": {
     "logged": 1627749024136
    },
    "id": "Hn-sjHYZDHLU"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dim,\n",
    "                    embeddings_initializer=tf.keras.initializers.Constant(\n",
    "                        embedding_matrix)))#,trainable=False\n",
    "model.add(Dropout(0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627749027855
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9150"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_classes=len(train_df.BROWSE_NODE_ID.unique())\n",
    "no_of_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gather": {
     "logged": 1627749029230
    },
    "id": "FLEToGmCb3ZF",
    "outputId": "5bf8855b-0d4d-4841-bc0e-92c51f6ada6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 300)         6000000   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, None, 300)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, None, 500)         450500    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 12000)             6012000   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 12000)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 9150)              109809150 \n",
      "=================================================================\n",
      "Total params: 122,271,650\n",
      "Trainable params: 122,271,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Conv1D(500,\n",
    "                 3,\n",
    "                 padding='valid'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(12000, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(no_of_classes,activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "gather": {
     "logged": 1627749084277
    },
    "id": "R9H3ExHlzXFR"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ya9rhe8QcVrV"
   },
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627749086300
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627733851591
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627743879570
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits (logits-model, labels-y)) \n",
    "#optimizer = tf.train. AdamOptimizer (learning_rate learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "gather": {
     "logged": 1627750813079
    },
    "id": "i1lpjeBMepE-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 640/5997 [==>...........................] - ETA: 22:06 - loss: 5.8218 - accuracy: 0.2479\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 00001: saving model to training_1/cp.ckpt\n",
      "1280/5997 [=====>........................] - ETA: 20:34 - loss: 4.9329 - accuracy: 0.3276\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 00001: saving model to training_1/cp.ckpt\n",
      "1920/5997 [========>.....................] - ETA: 18:00 - loss: 4.5129 - accuracy: 0.3681\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 00001: saving model to training_1/cp.ckpt\n",
      "2560/5997 [===========>..................] - ETA: 15:16 - loss: 4.2530 - accuracy: 0.3941\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 00001: saving model to training_1/cp.ckpt\n",
      "3200/5997 [===============>..............] - ETA: 12:28 - loss: 4.0716 - accuracy: 0.4125\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 00001: saving model to training_1/cp.ckpt\n",
      "3840/5997 [==================>...........] - ETA: 9:39 - loss: 3.9356 - accuracy: 0.4264\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 00001: saving model to training_1/cp.ckpt\n",
      "4480/5997 [=====================>........] - ETA: 6:49 - loss: 3.8283 - accuracy: 0.4374\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 00001: saving model to training_1/cp.ckpt\n",
      "5120/5997 [========================>.....] - ETA: 3:56 - loss: 3.7409 - accuracy: 0.4465\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 00001: saving model to training_1/cp.ckpt\n",
      "5760/5997 [===========================>..] - ETA: 1:03 - loss: 3.6679 - accuracy: 0.4540\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 00001: saving model to training_1/cp.ckpt\n",
      "5997/5997 [==============================] - 1701s 283ms/step - loss: 3.6436 - accuracy: 0.4564 - val_loss: 15.2921 - val_accuracy: 0.0719\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "epochs = 1\n",
    "\n",
    "#callbacks = [EarlyStopping(monitor='val_loss')]\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1,\n",
    "                                                 save_freq=5*batch_size)\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "hist_adam=model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_val,y_val),\n",
    "          callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.suptitle('Optimizer : Adam', fontsize=10)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.plot(hist_adam.history['loss'], color='b', label='Training Loss')\n",
    "plt.plot(hist_adam.history['val_loss'], color='r', label='Validation Loss')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p saved_model\n",
    "model.save('saved_model/my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Run all the cells above before you sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Run all the cells below after the training is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627751835811
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-31 17:15:31.848560: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-07-31 17:15:31.849641: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-07-31 17:15:31.905310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0001:00:00.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2021-07-31 17:15:31.905364: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-31 17:15:31.908555: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-07-31 17:15:31.908613: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-07-31 17:15:31.910363: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-07-31 17:15:31.910655: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-07-31 17:15:31.912714: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-07-31 17:15:31.913742: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-07-31 17:15:31.913941: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-07-31 17:15:31.915342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-07-31 17:15:31.915766: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-31 17:15:31.916060: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-07-31 17:15:31.916812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0001:00:00.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2021-07-31 17:15:31.916844: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-31 17:15:31.916888: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-07-31 17:15:31.916914: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-07-31 17:15:31.916940: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-07-31 17:15:31.916965: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-07-31 17:15:31.916991: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-07-31 17:15:31.917017: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-07-31 17:15:31.917044: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-07-31 17:15:31.918498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-07-31 17:15:31.918556: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-31 17:15:32.470685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-07-31 17:15:32.470720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-07-31 17:15:32.470730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-07-31 17:15:32.473074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10624 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0001:00:00.0, compute capability: 3.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 300)         6000000   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, None, 300)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, None, 500)         450500    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, None, 500)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, None, 250)         625250    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, None, 250)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, None, 100)         175100    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 12000)             1212000   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 12000)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 9150)              109809150 \n",
      "=================================================================\n",
      "Total params: 118,272,000\n",
      "Trainable params: 118,272,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('saved_model/my_model')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627751839442
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "test_df=pd.read_csv('dataset/test.csv',escapechar = \"\\\\\",quoting = csv.QUOTE_NONE)\n",
    "x = test_df['TITLE'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627751930777
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happened\n",
      "Happened\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "max_text_length = 400\n",
    "x_tokenizer =text.Tokenizer(max_features)\n",
    "x_tokenizer.fit_on_texts(list(x))\n",
    "print(\"Happened\")\n",
    "x_tokenized = x_tokenizer.texts_to_sequences(x)\n",
    "print(\"Happened\")\n",
    "x_test_val = sequence.pad_sequences(x_tokenized, maxlen=max_text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627751942705
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 5.4000e+01, 1.7000e+01,\n",
       "        1.8072e+04],\n",
       "       [0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 6.0000e+00, 3.0000e+01,\n",
       "        2.7300e+02],\n",
       "       [0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 1.4100e+02, 5.6000e+01,\n",
       "        2.7450e+03],\n",
       "       ...,\n",
       "       [0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 1.7000e+01, 1.0000e+01,\n",
       "        8.0000e+00],\n",
       "       [0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 1.0000e+01, 3.4800e+02,\n",
       "        1.8400e+02],\n",
       "       [0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 1.7000e+01, 1.0000e+01,\n",
       "        1.4000e+01]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_val=np.float32(x_test_val)\n",
    "x_test_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627752073781
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-31 17:19:32.725537: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-07-31 17:19:32.726066: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2596985000 Hz\n",
      "2021-07-31 17:19:32.846084: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-07-31 17:19:33.039336: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-07-31 17:19:33.051071: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-07-31 17:19:33.997781: W tensorflow/stream_executor/gpu/asm_compiler.cc:63] Running ptxas --version returned 256\n",
      "2021-07-31 17:19:34.070991: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2021-07-31 17:21:13.213920: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 4054365000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "pred1=new_model.predict(x_test_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627752726889
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "sub=pd.read_csv('dataset/sample_submission.csv',escapechar = \"\\\\\",quoting = csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627753771337
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "len(pred1[3])\n",
    "train_df=pd.read_csv('dataset/train.csv',escapechar = \"\\\\\",quoting = csv.QUOTE_NONE)\n",
    "le = LabelEncoder()\n",
    "y=le.fit_transform(train_df['BROWSE_NODE_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627753772237
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627754186367
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "pred=[]\n",
    "for item in range(len(pred1)):\n",
    "    ind = np.argpartition(pred1[item], -5)[-5:]\n",
    "    pred.append([ind[np.argsort(pred1[1,ind])][-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627753316171
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ind = np.argpartition(pred1[1,:], -5)[-5:]\n",
    "#ind[np.argsort(pred1[1,ind])][-1] #, pred1[1,:]\n",
    "#pred1[1,ind]#np.max(pred1[1,ind])\n",
    "#np.argmax(pred1[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627754299081
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 107,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 641,\n",
       " 4,\n",
       " 4,\n",
       " 2905,\n",
       " 1043,\n",
       " 4,\n",
       " 905,\n",
       " 890,\n",
       " 299,\n",
       " 4,\n",
       " 299,\n",
       " 4,\n",
       " 855,\n",
       " 65,\n",
       " 4,\n",
       " 4,\n",
       " 413,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 944,\n",
       " 4,\n",
       " 677,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 335,\n",
       " 4,\n",
       " 743,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1075,\n",
       " 4,\n",
       " 743,\n",
       " 4,\n",
       " 903,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 369,\n",
       " 63,\n",
       " 279,\n",
       " 4,\n",
       " 4,\n",
       " 935,\n",
       " 944,\n",
       " 4,\n",
       " 1200,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 494,\n",
       " 4,\n",
       " 683,\n",
       " 145,\n",
       " 64,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 47,\n",
       " 754,\n",
       " 4,\n",
       " 903,\n",
       " 903,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 911,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 677,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 63,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1075,\n",
       " 4,\n",
       " 335,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 660,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 963,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 808,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 1778,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 36,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 294,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1017,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 968,\n",
       " 679,\n",
       " 1769,\n",
       " 1493,\n",
       " 855,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1699,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 47,\n",
       " 4,\n",
       " 329,\n",
       " 4,\n",
       " 1479,\n",
       " 2417,\n",
       " 1696,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 667,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 683,\n",
       " 808,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 531,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 161,\n",
       " 944,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 677,\n",
       " 1261,\n",
       " 4,\n",
       " 156,\n",
       " 4,\n",
       " 4,\n",
       " 1075,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 677,\n",
       " 683,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 718,\n",
       " 4,\n",
       " 4,\n",
       " 3706,\n",
       " 5412,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1231,\n",
       " 4,\n",
       " 683,\n",
       " 683,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 649,\n",
       " 1019,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 677,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1769,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 60,\n",
       " 101,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3279,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 677,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1023,\n",
       " 956,\n",
       " 4,\n",
       " 4,\n",
       " 1869,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 956,\n",
       " 855,\n",
       " 4,\n",
       " 4,\n",
       " 440,\n",
       " 1010,\n",
       " 4,\n",
       " 2239,\n",
       " 4,\n",
       " 743,\n",
       " 3150,\n",
       " 131,\n",
       " 4,\n",
       " 2208,\n",
       " 427,\n",
       " 1608,\n",
       " 4,\n",
       " 656,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 683,\n",
       " 63,\n",
       " 4,\n",
       " 808,\n",
       " 372,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 743,\n",
       " 4,\n",
       " 4,\n",
       " 800,\n",
       " 1687,\n",
       " 2169,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 554,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 194,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2323,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 743,\n",
       " 4,\n",
       " 1023,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 694,\n",
       " 4,\n",
       " 194,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 161,\n",
       " 47,\n",
       " 4,\n",
       " 4,\n",
       " 677,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 388,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 813,\n",
       " 4,\n",
       " 4,\n",
       " 743,\n",
       " 22,\n",
       " 1988,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 63,\n",
       " 4,\n",
       " 4,\n",
       " 1407,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 891,\n",
       " 4,\n",
       " 4,\n",
       " 800,\n",
       " 1023,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 808,\n",
       " 107,\n",
       " 1339,\n",
       " 4,\n",
       " 1699,\n",
       " 1060,\n",
       " 4,\n",
       " 903,\n",
       " 4,\n",
       " 4,\n",
       " 691,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 101,\n",
       " 4,\n",
       " 4,\n",
       " 2323,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 649,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 335,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 903,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1540,\n",
       " 903,\n",
       " 4,\n",
       " 1023,\n",
       " 4,\n",
       " 60,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 677,\n",
       " 294,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 944,\n",
       " 4,\n",
       " 808,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 396,\n",
       " 4,\n",
       " 1075,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 784,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 743,\n",
       " 4,\n",
       " 4,\n",
       " 677,\n",
       " 4,\n",
       " 677,\n",
       " 683,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 743,\n",
       " 4,\n",
       " 4,\n",
       " 808,\n",
       " 4,\n",
       " 743,\n",
       " 743,\n",
       " 743,\n",
       " 1613,\n",
       " 743,\n",
       " 956,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 743,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 1367,\n",
       " 903,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 591,\n",
       " 4,\n",
       " 683,\n",
       " 4,\n",
       " 4,\n",
       " 678,\n",
       " 1200,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1023,\n",
       " 4,\n",
       " 677,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 668,\n",
       " 4,\n",
       " 677,\n",
       " 4,\n",
       " 4,\n",
       " 95,\n",
       " 4,\n",
       " 800,\n",
       " 4,\n",
       " 2554,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 590,\n",
       " 4,\n",
       " 4,\n",
       " 948,\n",
       " 4,\n",
       " 3202,\n",
       " 335,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 878,\n",
       " 4,\n",
       " 1987,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1415,\n",
       " 4,\n",
       " 4,\n",
       " 1380,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 335,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 530,\n",
       " 4,\n",
       " 4,\n",
       " 678,\n",
       " 4,\n",
       " 1494,\n",
       " 4,\n",
       " 891,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 908,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 808,\n",
       " 4,\n",
       " 4,\n",
       " 956,\n",
       " 161,\n",
       " 4,\n",
       " 47,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 299,\n",
       " 1432,\n",
       " 4,\n",
       " 4,\n",
       " 1075,\n",
       " 683,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 47,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 95,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 649,\n",
       " 4,\n",
       " 683,\n",
       " 3890,\n",
       " 226,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 743,\n",
       " 942,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 903,\n",
       " 4,\n",
       " 4,\n",
       " 427,\n",
       " 4,\n",
       " 4,\n",
       " 439,\n",
       " 4,\n",
       " 194,\n",
       " 4,\n",
       " 4,\n",
       " 688,\n",
       " 4,\n",
       " 656,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 471,\n",
       " 4,\n",
       " 1231,\n",
       " 656,\n",
       " 4,\n",
       " 4,\n",
       " 2391,\n",
       " 4,\n",
       " 4,\n",
       " 677,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 104,\n",
       " 1341,\n",
       " 4,\n",
       " 185,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2579,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 743,\n",
       " 731,\n",
       " 4,\n",
       " 855,\n",
       " 743,\n",
       " 4,\n",
       " 4,\n",
       " 691,\n",
       " 743,\n",
       " 808,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1148,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 677,\n",
       " 4,\n",
       " 335,\n",
       " 1700,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 194,\n",
       " 1403,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 60,\n",
       " 530,\n",
       " 427,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1101,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2323,\n",
       " 133,\n",
       " 656,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1060,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 743,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 194,\n",
       " 2550,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 95,\n",
       " 1534,\n",
       " 4,\n",
       " 4,\n",
       " 2059,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4231,\n",
       " 677,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 165,\n",
       " 4,\n",
       " 295,\n",
       " 905,\n",
       " 4,\n",
       " 194,\n",
       " 4,\n",
       " 4,\n",
       " 2059,\n",
       " 4,\n",
       " 441,\n",
       " 4,\n",
       " 4,\n",
       " 591,\n",
       " 649,\n",
       " 4,\n",
       " 4,\n",
       " 878,\n",
       " 1476,\n",
       " 4,\n",
       " 4,\n",
       " 1161,\n",
       " 4,\n",
       " 4,\n",
       " 2323,\n",
       " 4,\n",
       " 4,\n",
       " 887,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2579,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 301,\n",
       " 4,\n",
       " 4,\n",
       " 63,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 743,\n",
       " 299,\n",
       " 4,\n",
       " ...]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = [item for sublist in pred for item in sublist]\n",
    "#pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627754307284
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "y=le.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627754673212
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PRODUCT_ID', 'BROWSE_NODE_ID'], dtype='object')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627754765807
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(903024, 110775)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df),len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627754920358
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>BROWSE_NODE_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PRODUCT_ID  BROWSE_NODE_ID\n",
       "0           1               4\n",
       "1           2             122\n",
       "2           3               4\n",
       "3           4               4\n",
       "4           5               4"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'PRODUCT_ID': test_df.PRODUCT_ID, 'BROWSE_NODE_ID': y}\n",
    "SUBMISSION = pd.DataFrame(data=d)\n",
    "SUBMISSION.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1627755090140
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "SUBMISSION.to_csv('First_Submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "sequence_length = train_df.shape[1]\n",
    "filter_sizes = [3,4]\n",
    "num_filters = 100\n",
    "drop = 0.4\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((2*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "conc = Dense(40)(dropout)\n",
    "output = Dense(units=6, activation='sigmoid',kernel_regularizer=regularizers.l2(0.01))(conc)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "opt = Adam(lr=1e-3)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt)\n",
    "\n",
    "# Fitting Model to the data\n",
    "callbacks = [EarlyStopping(monitor='val_loss')]\n",
    "hist_adam = model.fit(X_train, y_train, batch_size=1000, epochs=20, verbose=2, validation_data=(X_val, y_val),\n",
    "         callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "sequences_test=tokenizer.texts_to_sequences(X_test)\n",
    "X_test2 = pad_sequences(sequences_test,maxlen=X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "col = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']\n",
    "\n",
    "# Predict on train, val and test datasets\n",
    "pred_train = model.predict(X_train)\n",
    "pred_test = model.predict(X_test2)\n",
    "pred_val = model.predict(X_val)\n",
    "\n",
    "# Emply array to collect AUC scores\n",
    "AUC = np.zeros((3,6))\n",
    "AUC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "from sklearn import metrics\n",
    "for i,x in enumerate(col):\n",
    "    auc = np.array([metrics.roc_auc_score(y_train[:,i], pred_train[:,i]),\n",
    "                    metrics.roc_auc_score(y_val[:,i], pred_val[:,i]),\n",
    "                    metrics.roc_auc_score(y_test[x], pred_test[:,i])])\n",
    "    print(x,\"Train AUC:\",auc[0],\", Val AUC:\",auc[1],\", Test AUC:\",auc[2])\n",
    "    AUC[:,i] = auc\n",
    "    \n",
    "avg_auc = AUC.mean(axis=1)\n",
    "print(\"Average Train AUC:\",avg_auc[0],\", Average Val AUC:\",avg_auc[1],\", Average Test AUC:\",avg_auc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGDRqcCQD2is"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Amazon_ML_Glove_Embedding_with_CNN.ipynb",
   "provenance": []
  },
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
