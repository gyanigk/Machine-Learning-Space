<!doctype html><html>
    <head>
      <meta http-equiv="content-type" content="text/html; charset=utf-8">
      <title>Week3</title>
      <meta name="generator" content="CherryTree">
      <link rel="stylesheet" href="res/styles3.css" type="text/css" />
    </head>
    <body><div class='page'><h1 class='title'>Week3</h1><br/>Logistic Regression<br />Classification problems<br />	EMAIL: spam / not spam<br />	Online Transactions : Fradulent or not<br />	tumor malignant / benign <br /><br /><br />binary classification problem 0,1 <br />multiple classification problem 0,1,2,3,4..<br /><a href=""><img src="images/17-1.png" alt="images/17-1.png" /></a><br /><a href=""><img src="images/17-2.png" alt="images/17-2.png" /></a><br /><br />not a good idea to use linear regression for classification problem<br /><a href=""><img src="images/17-3.png" alt="images/17-3.png" /></a><br /><br />Hypothesis Representation<br /><br />logistic Regression model<br />sigmoud/logistic function<br /><br />asymtotes at 0,1<br /><br />fit the parameters theta to make predictions<br /><a href=""><img src="images/17-4.png" alt="images/17-4.png" /></a><br /><br />Intrepretation of hypothesis output<br /><a href=""><img src="images/17-5.png" alt="images/17-5.png" /></a><br /><br />Decision Boundary <br /><a href=""><img src="images/17-6.png" alt="images/17-6.png" /></a><br /><a href=""><img src="images/17-7.png" alt="images/17-7.png" /></a><br /><a href=""><img src="images/17-8.png" alt="images/17-8.png" /></a><br /><br /><br /><br /><h3>The </h3><strong>decision boundary</strong><h3> is the line that separates the area where y = 0 and where y = 1. It is created by our hypothesis function.</h3><br /><a href=""><img src="images/17-9.png" alt="images/17-9.png" /></a><br /><h3>In this case, our decision boundary is a straight vertical line placed on the graph where x_1 = 5</h3><em><h3>x</h3></em><h3>1</h3><small>​</small><h3>=5, and everything to the left of that denotes y = 1, while everything to the right denotes y = 0.</h3><br /><h3>Again, the input to the sigmoid function g(z) (e.g. \theta^T X</h3><em><h3>θ</h3></em><em><h3>T</h3></em><em><h3>X</h3></em><h3>) doesn&#39;t need to be linear, and could be a function that describes a circle (e.g. z = \theta_0 + \theta_1 x_1^2 +\theta_2 x_2^2</h3><em><h3>z</h3></em><h3>=</h3><em><h3>θ</h3></em><h3>0</h3><small>​</small><h3>+</h3><em><h3>θ</h3></em><h3>1</h3><small>​</small><em><h3>x</h3></em><h3>12</h3><small>​</small><h3>+</h3><em><h3>θ</h3></em><h3>2</h3><small>​</small><em><h3>x</h3></em><h3>22</h3><small>​</small><h3>) or any shape to fit our data.</h3><br /><br />Logistic regression Model<br />Cost Function<br /><a href=""><img src="images/17-10.png" alt="images/17-10.png" /></a><br /><br />non-complex - non linear function from the sigmod substitution to h(x) makes it give many local minima<br /> <a href=""><img src="images/17-11.png" alt="images/17-11.png" /></a><br /><br />logistic regression cost function<br /><a href=""><img src="images/17-12.png" alt="images/17-12.png" /></a><br /><br />convexity analysis is part of the topics-  convex optimization problem<br /><a href=""><img src="images/17-13.png" alt="images/17-13.png" /></a><br /><br />Simplified Cost function and Gradient descent <br /><a href=""><img src="images/17-14.png" alt="images/17-14.png" /></a><br />derived from statistics from the principle of maximum likelihood estimation<br />simplifying the cost function<br /><a href=""><img src="images/17-15.png" alt="images/17-15.png" /></a><br /><br />we use gradient descent to minimize the cost function<br /><a href=""><img src="images/17-16.png" alt="images/17-16.png" /></a><br /><a href=""><img src="images/17-17.png" alt="images/17-17.png" /></a><br /><br />feature scaling another approach<br /><br /><a href=""><img src="images/17-18.png" alt="images/17-18.png" /></a><br /><br />Advanced Optimization<br />	- advance numerical computing<br />	<br />finding a good library for any of these implementation of the advance or normal functions usage to yeild better and faster result<br /><a href=""><img src="images/17-19.png" alt="images/17-19.png" /></a><br />fminunc - function minimization unconstrainted<br /><a href=""><img src="images/17-20.png" alt="images/17-20.png" /></a><br /><br />options = optimset(&#39;GradObj&#39;,&#39;on&#39;,&#39;MaxIter&#39;,&#39;100&#39;);<br /><a href=""><img src="images/17-21.png" alt="images/17-21.png" /></a><br />cost function for logistic regression from gradient descent<br /><a href=""><img src="images/17-22.png" alt="images/17-22.png" /></a><br /><a href=""><img src="images/17-23.png" alt="images/17-23.png" /></a><br /><br />Multiclass Classificiation - one vs All<br /><a href=""><img src="images/17-24.png" alt="images/17-24.png" /></a><br /><a href=""><img src="images/17-25.png" alt="images/17-25.png" /></a><br /><a href=""><img src="images/17-26.png" alt="images/17-26.png" /></a><br /><a href=""><img src="images/17-27.png" alt="images/17-27.png" /></a><br />Whatever value gives the highest probability, we consider that class<br /><a href=""><img src="images/17-28.png" alt="images/17-28.png" /></a><br /><a href=""><img src="images/17-29.png" alt="images/17-29.png" /></a><br /><br /><br />Problem of Overfitting <br /><br /><a href=""><img src="images/17-30.png" alt="images/17-30.png" /></a><br />fit a linear function - not a great model - underfitting -  high bias<br />quadratic function - better model - just right<br />polynomial function - overfitting - high variance<br />	- fails to generalize the new examples<br /><a href=""><img src="images/17-31.png" alt="images/17-31.png" /></a><br /><br />addressing overfitting<br />- problems due to alot of features and cause problems in plotting and visualization<br />- smaller training set and alot of faetures - causes overfitting<br /><a href=""><img src="images/17-32.png" alt="images/17-32.png" /></a><br />options to overcome the problem :<br /><a href=""><img src="images/17-33.png" alt="images/17-33.png" /></a><br /><br />Regularization - Cost Function<br /><a href=""><img src="images/17-34.png" alt="images/17-34.png" /></a><br /><br />minimizing the function with more parameter by nulling them to zeros<br />	shrink all the parameters 	<br /><a href=""><img src="images/17-35.png" alt="images/17-35.png" /></a><br /><a href=""><img src="images/17-36.png" alt="images/17-36.png" /></a><br /><a href=""><img src="images/17-37.png" alt="images/17-37.png" /></a><br /><br />Regularized Linear Regression<br /><a href=""><img src="images/17-38.png" alt="images/17-38.png" /></a><br /><br /><a href=""><img src="images/17-39.png" alt="images/17-39.png" /></a><br /><a href=""><img src="images/17-40.png" alt="images/17-40.png" /></a><br />this matrix will not be singular (...)<br /><h3>Recall that if m &lt; n, then X^TX</h3><em><h3>X</h3></em><em><h3>T</h3></em><em><h3>X</h3></em><h3> is non-invertible. However, when we add the term λ⋅L, then X^TX</h3><em><h3>X</h3></em><em><h3>T</h3></em><em><h3>X</h3></em><h3> + λ⋅L becomes invertible.</h3><br /><br />Regularized Logistic Regression<br /><a href=""><img src="images/17-41.png" alt="images/17-41.png" /></a><br /><a href=""><img src="images/17-42.png" alt="images/17-42.png" /></a><br />advanced optimization methods<br /><a href=""><img src="images/17-43.png" alt="images/17-43.png" /></a><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />	</div></body></html>