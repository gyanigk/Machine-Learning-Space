<!doctype html><html>
    <head>
      <meta http-equiv="content-type" content="text/html; charset=utf-8">
      <title>Week2</title>
      <meta name="generator" content="CherryTree">
      <link rel="stylesheet" href="res/styles3.css" type="text/css" />
    </head>
    <body><div class='page'><h1 class='title'>Week2</h1><br/>  Multiple features  (variables)<br />	x1 x2 x3 x4 y<br />	.<br />	. <br />	.<br />	m times (m=47)<br /><br />n :number of features ( like size, no of bedrooms, no of floors, age of home)<br />x(i) :input (features) of ith training example<br />x(j,i) :value of feature of j in ith training example<br /><br />	hypothesis<br />h(x) =  O1 + O2*x earlier<br />but now, h(x) = O1 + O2*x1 + O3*x2 + ...<br /><br />Linear regression with multiple variables is also known as multivariate linear regression<br /><em>x</em>(<em>i</em>) =value of feature <em>j</em> in the <em>ith</em> training example<br /><em>jx</em>(<em>i</em>) =the input (features) of the <em>ith</em> training example<br /><em>m =the number of training examples<br />n</em> =the number of features<br /><br /><em>h</em><em>θ</em>​(<em>x</em>)=<em>θ</em>0​+<em>θ</em>1​<em>x</em>1​+<em>θ</em>2​<em>x</em>2​+<em>θ</em>3​<em>x</em>3​+⋯+<em>θ</em><em>n</em>​<em>x</em><em>n</em>​<br /><h3>vectorization of our hypothesis function</h3><br /><br /><br />gradient descent for multiple variable <br /><a href=""><img src="images/16-1.png" alt="images/16-1.png" /></a><br /><br /><br />Feature scaling : gradient descent in practice I<br /><br />idea : make sure features are on a similar scale<br />	i.e x1 = size ( 0-2000 )<br />		x2 = no of bedrooms (1-5)<br />			if we plot the contour of the cos function J<em>θ</em><br />			<br />	<a href=""><img src="images/16-2.png" alt="images/16-2.png" /></a><br />			on the left image, the global minima takes more time to be found. due to narrow contour lines and variable gaps<br />			where as the image on the right will be give better solutiona as it equally arranged for the minima <br />			<br />we should get every feature into approx. a -1 &lt;= x(i) &lt;= 1 range<br />make sure the range is [-1,1] and have reasonale decimal value<br /><br />Another way is to use :<br />Mean Normalization<br />Replace the x(i) with x(i) - U(i) to make features have approx zero mean ( do not apply x(0) =1)	<br /><a href=""><img src="images/16-3.png" alt="images/16-3.png" /></a><br /><br />Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero<br /><br />Learning rate  : gradient descent in practice II<br />debugging to make sure the gradient descent is working  i.e choose learning rate <br /><a href=""><img src="images/16-4.png" alt="images/16-4.png" /></a><br /><br /><a href=""><img src="images/16-5.png" alt="images/16-5.png" /></a><br /><br /><br />if a is too small : slow convergence<br /> ...0.001... 0.01...0.1....1...<br /><br />Features and Polynomial regression<br />	suppose two features are length and breadth<br />		we can consider one feature Area for the above ; understand the need of and relation of features for optimization<br />		<a href=""><img src="images/16-6.png" alt="images/16-6.png" /></a><br />	<br />	problem : A quadratic function is increasing but may decrease later<br />So, implement different choice of features or implement change in function<br />choice of feature<br /><a href=""><img src="images/16-7.png" alt="images/16-7.png" /></a><br /><br /><br />Computing Parameters analytically<br />Normal Equation<br />method to solve for Theta analytically - i.e in one step instead of iterative <br />In the &quot;Normal Equation&quot; method, we will minimize J by explicitly taking its derivatives with respect to the θj ’s, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below: <br /><em>θ</em>= (X^T * X)^(-1) * (X^T) *y<br /><br /><br /><a href=""><img src="images/16-8.png" alt="images/16-8.png" /></a><br /><br /><a href=""><img src="images/16-9.png" alt="images/16-9.png" /></a><br />no need of feature scaling with normal equation<br /><br /><a href=""><img src="images/16-10.png" alt="images/16-10.png" /></a><br /><br /><br />Normal Equation Invertibity<br /><br />if the matrix is invertible ( X^T * X )<br />	this is caused due to :<br />1.	reduntant features (linearly dependent)<br />	 two features are very closely related<br />2. too many features (m&gt;=n)	<br />   delete some features or use regularization<br /><br />When implementing the normal equation in octave we want to use the &#39;pinv&#39; function rather than &#39;inv.&#39; The &#39;pinv&#39; function will give you a value of \theta<em>θ</em> even if X^TX<em>X</em><em>T</em><em>X</em> is not invertible<br /><br /></div></body></html>