<!doctype html><html>
    <head>
      <meta http-equiv="content-type" content="text/html; charset=utf-8">
      <title>Week5</title>
      <meta name="generator" content="CherryTree">
      <link rel="stylesheet" href="res/styles3.css" type="text/css" />
    </head>
    <body><div class='page'><h1 class='title'>Week5</h1><br/>Neural network learning<br /><br />application of nn in classification problem<br /><a href=""><img src="images/19-1.png" alt="images/19-1.png" /></a><br /><br />Cost function<br />generalization of logistic regression <br />we do not regularise the biased term - 1...n<br /><br />K=1..4 - 4 output units<br />summing the cost function on each of them <br /><br />y(k) , h(k)<br /><br />regularization term - lambda , for all values of i,j,l<br /><a href=""><img src="images/19-2.png" alt="images/19-2.png" /></a><br /><br />algorithm the minimize the cost function<br />or backpropagation algorithm<br /><a href=""><img src="images/19-3.png" alt="images/19-3.png" /></a><br /><br />a(1) is the activation value or layer 1<br />z2 is required value for sigmoid function in the next layer<br /><a href=""><img src="images/19-4.png" alt="images/19-4.png" /></a><br /><br />each node we compute a new delta j(l) is error of node j and layer l<br />L=4<br />for each layer we calculate the output unit<br /><br />we have derivative of the sigmoid function in delta calculation<br /><a href=""><img src="images/19-5.png" alt="images/19-5.png" /></a><br /><br />training set of m examples<br />traingle is capital delta<br />for ith iteration we are working with x(i),y(i)<br />	perform forward propagation<br />		activation calucalation<br />	output layer - a(l) output  of hypothesus layer minus target label was<br />	we use the capital delta terms to accumulate the partial derivatives<br />	<br />we calculate seperate terms for j=0,j!=0<br />j=0 refers to the biased term taken in regard so no regularization terms<br /><a href=""><img src="images/19-6.png" alt="images/19-6.png" /></a><br /><br />Backpropagation Intuition<br /><br />we use sigmoid function to get the activation values <br /><a href=""><img src="images/19-7.png" alt="images/19-7.png" /></a><br />beautifully explained the forward calculation over each layers and nodewise<br /><br />cost term is associated with the ith term of the x(i),y(i)<br />cost(i) = (neural network value h(x) - actual value y(i))^2 to present only postive values <br /><a href=""><img src="images/19-8.png" alt="images/19-8.png" /></a><br /><br />delta terms are partial derivatives of the cost function with respect to the neural network values of each layer of given specific example<br /><br />delta value is calulated by multiply the edge weight with the delta value of the previous layer (D(i+1) = D(i)*theta(i+1)(i,i+1,...) + D(i)*theta(i+1)(i,i,...)<br />bais unit just output the value 1<br /><a href=""><img src="images/19-9.png" alt="images/19-9.png" /></a><br /><br />implementation note: unrolling parameters <br /><br />from matrixes to vectors<br />advance optimization<br /><br />take input of pointed to cost function and some initial value of theta and options<br />this was fine for logistic regression<br />but in neural nets we have these matrices for theta terms, or gradient terms<br /><br />so, we to present the matrixes into vectors so that they can be properly implemented in the optimization ~ Unroll the vectors<br /><a href=""><img src="images/19-10.png" alt="images/19-10.png" /></a><br /><br />s(i) = units in layer i<br />dimensions are expressed <br />all the elements of matrices will become one large vector<br /><br />we use reshape with no of elements to pull out<br /><a href=""><img src="images/19-11.png" alt="images/19-11.png" /></a><br /><a href=""><img src="images/19-12.png" alt="images/19-12.png" /></a><br /><a href=""><img src="images/19-13.png" alt="images/19-13.png" /></a><br /><a href=""><img src="images/19-14.png" alt="images/19-14.png" /></a><br /><a href=""><img src="images/19-15.png" alt="images/19-15.png" /></a><br /><br /><a href=""><img src="images/19-16.png" alt="images/19-16.png" /></a><br />implementation algo with required return value types as vectors <br /><a href=""><img src="images/19-17.png" alt="images/19-17.png" /></a><br /><br />Gradient Checking<br />it look like its working, cost function is decreasing with every implementation of gradient descent. there could be a bug that would create problems <br />so, we use gradient checking if there is any problem in the given training process for forward and backpropagation <br />verify the code is working<br /><br />derivative - numerical process of take limit values for approximation of the slope/tangent - derivative or gradient value<br />there are large value of episolon to be considered <br />if the value of episolon is taken very less then, it almost becomes the exact value of the paartial derivative of the J(theta)- there is now a numerical problem <br /><br />one-sided difference gives less accuracy<br />two-sided gives better accuracy<br /><a href=""><img src="images/19-18.png" alt="images/19-18.png" /></a><br /><br />consider theta as vector<br /><a href=""><img src="images/19-19.png" alt="images/19-19.png" /></a><br />approximate the value of J wrt to theta(i)<br /><a href=""><img src="images/19-20.png" alt="images/19-20.png" /></a><br /><br />once the implementation fo backprop is verified via gradient checking, we should stop using gradient checking else the code becomes slower<br /><br />numerical gradient checking code is much slower than the back prop. algo<br /><a href=""><img src="images/19-21.png" alt="images/19-21.png" /></a><br /><br />Random initialization<br /><br />initializing the theta parameters zeros is okay with logistic regression <br />but doesnt work with NN<br />hidden layer would give same values of the function as the all the initial theta are same and zero<br /><a href=""><img src="images/19-22.png" alt="images/19-22.png" /></a><br /><a href=""><img src="images/19-23.png" alt="images/19-23.png" /></a><br /><a href=""><img src="images/19-24.png" alt="images/19-24.png" /></a><br /><br />Putting it together<br /><br />choices are archietectural choices<br /><br />no of input units : dimensions of features<br />no of output units : no of classes : y : 1,2,3......K<br /><br />same no units in every single hidden layer	?<br />increasing the number of hidden layers makes it more computationationaly expensive	- more time - slower <br /><a href=""><img src="images/19-25.png" alt="images/19-25.png" /></a><br /><br />backprop - for loop to iterate over the examples<br />	for first time<br /><a href=""><img src="images/19-26.png" alt="images/19-26.png" /></a><br /><br />advance optimization - can get stuck in local optima<br />usually gradient descent works fine<br /><a href=""><img src="images/19-27.png" alt="images/19-27.png" /></a><br /><br /><a href=""><img src="images/19-28.png" alt="images/19-28.png" /></a><br /><br />Autonomous Driving<br />- getting a car to learn to drive<br /><br />direction of selected by human driver<br />left 					right<br />NN direction selected <br />left 					right<br /><br />caamera view of the car 						<br /><br /></div></body></html>