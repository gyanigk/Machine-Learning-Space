<!doctype html><html>
    <head>
      <meta http-equiv="content-type" content="text/html; charset=utf-8">
      <title>Week6</title>
      <meta name="generator" content="CherryTree">
      <link rel="stylesheet" href="res/styles3.css" type="text/css" />
    </head>
    <body><div class='page'><h1 class='title'>Week6</h1><br/>Advice for applying Machine Learning<br /><br />Deciding what to try next<br /><br />really know how to know to implement the algorithms<br />make sure the ml systems, to the most promising avenues to invest time<br />improve the ML system<br /><br />getting more data might not be so helpful always<br />trying to use smaller set of features to avoid overfitting<br />trying getting additional features<br />trying adding polynomial features<br /><a href=""><img src="images/20-1.png" alt="images/20-1.png" /></a><br /><a href=""><img src="images/20-2.png" alt="images/20-2.png" /></a><br /><br />Evaluating A hypothesis<br />prevent the problems of overfitting and underfitting<br /><br />low training error doesnt mean better hypothesis<br />failing to generalize the function<br /><a href=""><img src="images/20-3.png" alt="images/20-3.png" /></a><br /><br /> split the data into two portions <br />	i.e 70/30 split<br /><a href=""><img src="images/20-4.png" alt="images/20-4.png" /></a><br /><br />learn the parameter from train data<br /><br />compute test set error<br /><a href=""><img src="images/20-5.png" alt="images/20-5.png" /></a><br /><br />Model Selection and training/validation/test_sets<br />What ?<br />degree of polynomial to consider<br />which features to consider<br />choose the regularization lambda value for learning algorithms<br />these are model selection problem<br /><br /><a href=""><img src="images/20-6.png" alt="images/20-6.png" /></a><br /><br /><br />choose a model that wouldfit the model, also estimate the model gives fitted output<br />overly optimistic generalization layer<br /><a href=""><img src="images/20-7.png" alt="images/20-7.png" /></a><br /><br />what is cross-validation purpose? ~validation set<br /><a href=""><img src="images/20-8.png" alt="images/20-8.png" /></a><br /><a href=""><img src="images/20-9.png" alt="images/20-9.png" /></a><br />instead of using test set for model selection, we use validation set for model selection<br />pick the lowest vaalidation error<br /><a href=""><img src="images/20-10.png" alt="images/20-10.png" /></a><br /><br />Diagnosing bias vs Variance<br />underfitting or overfitting problem<br /><a href=""><img src="images/20-11.png" alt="images/20-11.png" /></a><br /><br />given below of training error graph. the error steeps more quickly than depicted<br /><a href=""><img src="images/20-12.png" alt="images/20-12.png" /></a><br /><br />my doubt is how to select the order of features? coz each linear function with different feaature would give different slope of error, and considering a set of features would be tidious first hand. <br />now in error and validation error graph. the curve of cross validation could give more than one minima or what?<br /><br />low order polynomial - high bias<br />high order polynomial - high variance<br /><a href=""><img src="images/20-13.png" alt="images/20-13.png" /></a><br /><br /><br />Regularization and Bias/Variance<br /><br />effect of regularization on bias/variaance<br /><br /><a href=""><img src="images/20-14.png" alt="images/20-14.png" /></a><br /><br /><a href=""><img src="images/20-15.png" alt="images/20-15.png" /></a><br /><br />take all the vaalues and put them on cross validation set, based on average sq error<br />pick any one which gives lowest cv error value<br />take the paramater and test how it works on test set<br /><a href=""><img src="images/20-16.png" alt="images/20-16.png" /></a><br /><br />compare the J train and J cv<br />small lambda, not using regularization<br />large lambda- problem of bias - use regularization<br /><br />bais-underfitting<br />on cv we find better variance and bias <br /><a href=""><img src="images/20-17.png" alt="images/20-17.png" /></a><br />on real problems, we could find similar trends, but we can find alot of noise sometimes<br /><br />Learning curves<br />sanity check of algorithms, diagonizing the probelm<br />plot J(train) or J(cv) as a function of m (training examples)<br />artificially reduce set ize<br />limit the training examples <br /><br />m = 1,2,3 then we have error = zero w/out regularization<br />											 = ~zero w/ regularization<br />		smaller the training set, smaller error<br />average training set error grows with more training set size		<br /><a href=""><img src="images/20-18.png" alt="images/20-18.png" /></a><br /><br /><br />straight line is pretty much the same<br />high bias, the Jtrain will come close to Jcv<br />problem is high error is persistent<br /><br /><a href=""><img src="images/20-19.png" alt="images/20-19.png" /></a><br /><br />J train will lower<br />J cv will remain higher to high<br />there is a large gap<br /><a href=""><img src="images/20-20.png" alt="images/20-20.png" /></a><br /><br />Deciding what to do next revisited <br /><a href=""><img src="images/20-21.png" alt="images/20-21.png" /></a><br /> <br />regularization is good for large NN<br /><a href=""><img src="images/20-22.png" alt="images/20-22.png" /></a><br /><br /><br />Machine Learning System Design<br /><br />Prioritizing What to work on : Spam classification example<br /><a href=""><img src="images/20-23.png" alt="images/20-23.png" /></a><br /><br />building a spam classifier :<br />given x and y we caan train using logistic regression<br /><br /><br /><a href=""><img src="images/20-24.png" alt="images/20-24.png" /></a><br /><a href=""><img src="images/20-25.png" alt="images/20-25.png" /></a><br /><br />Error Analysis<br />helps in avoid premature optimization<br /><a href=""><img src="images/20-26.png" alt="images/20-26.png" /></a><br /><br /><br />type and features considered are important for analysis hereS<br /><a href=""><img src="images/20-27.png" alt="images/20-27.png" /></a><br /><br />numerical evaluation of learning algo<br /><a href=""><img src="images/20-28.png" alt="images/20-28.png" /></a><br />stemming - takes the words of verb form/pluralform/continuous tense form/ as one word<br /><a href=""><img src="images/20-29.png" alt="images/20-29.png" /></a><br /><br />error analysis on cv set rather that test set<br />It is very important to get error results as a single, numerical value. Otherwise it is difficult to assess your algorithm&#39;s performance. For example if we use stemming, which is the process of treating the same word with different forms (fail/failing/failed) as one word (fail), and get a 3% error rate instead of 5%, then we should definitely add it to our model. However, if we try to distinguish between upper case and lower case letters and end up getting a 3.2% error rate instead of 3%, then we should avoid using this new feature.  Hence, we should try new things, get a numerical value for our error rate, and based on our result decide whether we want to keep the new feature or not. <br /><br />Handling Skewed Data<br />error metrics for skewed classes<br /><br />in the below image, we see the case of positive example is much much smaller than the number of negative examples beacuse y equals one so rarely;<br />this is what a skewed class is.<br />lot more examples from one class than other class<br />it becomes much harder to use just clasification accuracy, becuase you can get very high classification accurcies or very low errors nd its not always clear if doing so is really improving the quality of classifier. So to get a clear image on this, we need a different error metrics; like precision/recall<br /><a href=""><img src="images/20-30.png" alt="images/20-30.png" /></a><br />True positive<br />True negative<br />False positive<br />False negative<br /><br />precision<br />total number in the first row are the predicted class true y=1<br />recall - of all the patients that actually have cancer, how many we have predicted have cancer<br /><br />this will give a indication of how our classifier is doing<br /><a href=""><img src="images/20-31.png" alt="images/20-31.png" /></a><br /><br /><a href=""><img src="images/20-32.png" alt="images/20-32.png" /></a><a href=""><img src="images/20-33.png" alt="images/20-33.png" /></a> precision<br /><br /><a href=""><img src="images/20-34.png" alt="images/20-34.png" /></a><a href=""><img src="images/20-35.png" alt="images/20-35.png" /></a>recall<br />precision and recall are  defined setting y equals  1, rather than y equals 0, to be sort of  that the presence of that rare <br />class that we&#39;re trying to detect. And in particular, if a classifier is getting high precision and high  recall, then we are  actually confident that the algorithm has to be doing well, even if we have very skewed classes. So for the problem of skewed classes precisionrecall gives us more  direct insight into how  the learning algorithm is doing and this is often a much better way to evaluate our learning algorithms, than looking at classification error or classification accuracy, when the classes are very skewed.<br /><br />Trading off precision and recall<br /><br />Choosing higher accuracy rate as .9 or .7,this will affect the outcome we wish to find. so this would be a higher precision classifier will have lower recall because  we want to correctly detect that those patients have cancer<br />another problem is to avoid too many false negatives. the above approach may seem to solve problem but the false negatives are really not an option to skip<br />there is a doubt , so it better to report it as cancer. so we might instead set the value to lower level like .3 and be conservative that appropriate to seek for treatment. so this would be a higher recall classifier and going to be flaggin a higher fraction of all of the patients that actually do have cancer<br /><a href=""><img src="images/20-36.png" alt="images/20-36.png" /></a><br />the precision recall curve can look very different shapes<br /><br />Is there a way to choose this values automatically ? how to decide what is best among different approachs<br />taking average is not so good for any algo <br />so we look at F1 score - generally used in ML<br /><a href=""><img src="images/20-37.png" alt="images/20-37.png" /></a><br /><a href=""><img src="images/20-38.png" alt="images/20-38.png" /></a><br /><br />Using Large Datasets<br />Data for ML<br /><br />Getting alot of data and training on a certain tye of learning algorithm, can be a very effective way to get a learning algorithm to do very good performance<br /><a href=""><img src="images/20-39.png" alt="images/20-39.png" /></a><br /><a href=""><img src="images/20-40.png" alt="images/20-40.png" /></a><br />just knowing one feature doesnt make you feel the correct value. be it a ML learning algo or a human expert<br /><a href=""><img src="images/20-41.png" alt="images/20-41.png" /></a><br /><a href=""><img src="images/20-42.png" alt="images/20-42.png" /></a><a href=""><img src="images/20-43.png" alt="images/20-43.png" /></a><br /><br />training error will be close to test error - unlikely to overfit<br />large training set - low variance is considered<br />can a human expert look at the features x and confidently predict the value y ? one of the most important things to rem while solving the problem<br /><br /><a href=""><img src="images/20-44.png" alt="images/20-44.png" /></a><a href=""><img src="images/20-45.png" alt="images/20-45.png" /></a><br /><a href=""><img src="images/20-46.png" alt="images/20-46.png" /></a><a href=""><img src="images/20-47.png" alt="images/20-47.png" /></a><br /><a href=""><img src="images/20-48.png" alt="images/20-48.png" /></a><br /><br />The classifier achieves 99% accuracy because of the skewed classes in the data, not because it is overfitting the training set. Thus, it is likely to perform just as well on the cross validation set.<br /><br /><br /><br /><br /></div></body></html>