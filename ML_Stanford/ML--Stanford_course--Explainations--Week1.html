<!doctype html><html>
    <head>
      <meta http-equiv="content-type" content="text/html; charset=utf-8">
      <title>Week1</title>
      <meta name="generator" content="CherryTree">
      <link rel="stylesheet" href="res/styles3.css" type="text/css" />
    </head>
    <body><div class='page'><h1 class='title'>Week1</h1><br/>Week1<br />Machine Learning<br />A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E<br /><br />E = the experience of playing many games of checkers<br />T = the task of playing checkers.<br />P = the probability that the program will win the next game.<br /><br />classified : supervised learning or unsupervised learning <br /><br />Supervised Learning<br />	we are given a dataset and already know what our correct output should look like,<br />	having the idea that there is a relationship between the input-output<br />	- regression 	<br />	(predict results within a continuos output, map input variables to some continuos function)	<br />	- classification<br />	(predict results in a discrete output, map input variables to discrete categories)<br />		also called clustering algorithm<br /><br />unsupervised learning <br />	allows to approach problems with little or no idea what our results look like<br />		derive structure from data where we don&#39;t know variable effects<br />	derive this structure clustering based on reltionships among variables<br />	<br />Model and Cost Function<br /><br />Linear regression<br />	m = no of training examples<br />	x&#39;s input var / features<br />	y&#39;s output var / target variable<br />	(x,y) - one training example<br />	(x(i), y(i) ) - ith training example<br /><br />		   Training set<br />					|<br />		Learning Algorithms<br />					|<br />		X	&lt;- 	h  	- &gt; Y<br />				hypothesis : h function maps from X&#39;s to Y&#39;s<br /><br />Y vs X  graph, determine h(x)	- Linear regressiom wit one variable(univariate linear regression)<br />h : X-&gt;Y so that h(c) is a good predictor for corresponding value of y <br /><br />accuracy of the hypothesis function is measures by using cost function - average difference of all the results in hypothesis<br /><br />J(O1,O2) = 1/2m ( Summation ( y&#39;(i) - y(i) )^2 <br />		y&#39;(i) = h(x(i))<br /> minimize the difference between the predicted value and actual value ( mean squared error )<br />our objective is to get the best possible line so that avarge vertical distances of the scattered points from the line will be the least! <br /><br /><a href=""><img src="images/15-1.png" alt="images/15-1.png" /></a><br /><br /> <br />Contour plot - a graph that contains many contour lines <br /> 		- it is a two variable function has a constanat value at all the points of the same line<br />  		- same value of cost around consider single circle/ contour<br />  <a href=""><img src="images/15-2.png" alt="images/15-2.png" /></a><br /><br />Gradient descent<br /><a href=""><img src="images/15-3.png" alt="images/15-3.png" /></a><br /><br /><br />we need to estimate the parameters in the hypothesis function<br />Red arrow shows the minimum points in the graph<br />based on the parameter range J(O1,O2), we find the cost function values <br />using a tangential line to a function(derivative of cost function), show increasing or decreasing condition, we follow this to direct towards the steepest descent.<br />size of each step is determined by the parameter ( α alpha) called the learning rate<br />a smaller learning rate would be smaller step, and visa versa<br /><br />direction is determined - using partial derivative of the J(O1,O2) until convergence <br />	<a href=""><img src="images/15-4.png" alt="images/15-4.png" /></a><br /><br /><br /><em><h3>θ</h3></em><h3>1</h3><small>​</small><h3>:=</h3><em><h3>θ</h3></em><h3>1</h3><small>​</small><h3>−</h3><em><h3>α</h3></em><em><h3> </h3></em><em><h3>dθ</h3></em><h3>1</h3><small>​</small> <em><h3>d</h3></em><small>​</small><em><h3>J</h3></em><h3> (</h3><em><h3>θ</h3></em><h3>1</h3><small>​</small><h3>)</h3> - single parameter<br />More imporatantly <br /><a href=""><img src="images/15-5.png" alt="images/15-5.png" /></a><br /><a href=""><img src="images/15-6.png" alt="images/15-6.png" /></a><br />The problem of gradient descent is different local optima (steepest/ lowest points) <br />cost function of linear regression is always a bow shaped function :<br /><a href=""><img src="images/15-7.png" alt="images/15-7.png" /></a><br />this is called convex function.<br />because of this, the problem of different local optima is not present, as it has single global optima<br /><br /><a href=""><img src="images/15-8.png" alt="images/15-8.png" /></a><br /><br /><br /><br />applying gradient descent with linear regression<br />applying multivariable calculus<br /><a href=""><img src="images/15-9.png" alt="images/15-9.png" /></a><br /><a href=""><img src="images/15-10.png" alt="images/15-10.png" /></a><br /><br />repeat until convergence: <br />		{<br />			<em>θ</em>0:=<em>θ0−α1/m∑ [i=1...m] ( hθ(xi)−yi )<br />			θ</em>1:=<em>θ</em>1−<em>α</em>1/<em>m</em>∑ [<em>i</em>=1...<em>m</em>] ( (<em>hθ</em>(<em>xi</em>)−<em>yi</em>) <em>xi</em>)<br />		}	<br /><br />how the linear regression works if any point int he contour is consider and slowly it adjusted according to the center of the contour lines with h(x)<br /><a href=""><img src="images/15-11.png" alt="images/15-11.png" /></a><br /><a href=""><img src="images/15-12.png" alt="images/15-12.png" /></a><br /><br />this is called batch gradient descent, <br />	batch - each step of graident descent uses all the training examples <br />		∑ [<em>i</em>=1...<em>m</em>] ( (<em>hθ</em>(<em>xi</em>)−<em>yi</em>) <em>xi</em>) this summation from 1 to m is what makes this called the batch<br /></div></body></html>