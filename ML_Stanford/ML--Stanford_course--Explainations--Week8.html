<!doctype html><html>
    <head>
      <meta http-equiv="content-type" content="text/html; charset=utf-8">
      <title>Week8</title>
      <meta name="generator" content="CherryTree">
      <link rel="stylesheet" href="res/styles3.css" type="text/css" />
    </head>
    <body><div class='page'><h1 class='title'>Week8</h1><br/>Unsupervised Learning : Introduction<br /><br />Clustering <br /><br />learn from unlabeled data - pretty exciting<br /><a href=""><img src="images/22-1.png" alt="images/22-1.png" /></a><br /><br />clustering <br /><a href=""><img src="images/22-2.png" alt="images/22-2.png" /></a><br /><a href=""><img src="images/22-3.png" alt="images/22-3.png" /></a><br /><br />K means algorithm<br /><br />If I run the K Means clustering algorithm, here is what <br />I&#39;m going to do. The first step is to randomly initialize two points, called the cluster centroids. <br />So, these two crosses here, these are called the Cluster Centroidsand I have two of them <br />because I want to group my data into two clusters.<br />K Means is an iterative algorithm and it does two things.<br />First is a cluster assignment step, and second is a move centroid step. So, let me tell you what those things mean.<br /><br />The first of the two steps in the loop of K means, is this cluster assignment step. <br />What that means is that, it&#39;s going through each of the examples, each of these green dots shown here and depending on whether it&#39;s closer to the red cluster centroid or the blue cluster centroid, it is going to assign each of the data points to one of the two cluster centroids.<br /><br />Specifically, what I mean by that, is to go through your data set and color each of the points either red or blue, depending on whether it is closer to the red cluster centroid or the blue cluster centroid, and I&#39;ve done that in this diagram here.<br /><a href=""><img src="images/22-4.png" alt="images/22-4.png" /></a><br /><a href=""><img src="images/22-5.png" alt="images/22-5.png" /></a><br /><a href=""><img src="images/22-6.png" alt="images/22-6.png" /></a><br /><a href=""><img src="images/22-7.png" alt="images/22-7.png" /></a><br /><br /> if you keep running additional iterations of K means from here the cluster centroids will not change any further and the colours of the points will not change any further. And so, this is the, at this point, K means has converged and it&#39;s done a pretty good job findingthe two clusters in this data<br /><br /> if you keep running additional iterations of K means from here the cluster centroids will not change any further and the colours of the points will not change any further. And so, this is the, at this point, K means has converged and it&#39;s done a pretty good job findingthe two clusters in this data<br /><br /><a href=""><img src="images/22-8.png" alt="images/22-8.png" /></a><br /><br /><a href=""><img src="images/22-9.png" alt="images/22-9.png" /></a><br /><br /><a href=""><img src="images/22-10.png" alt="images/22-10.png" /></a><br /><br />Optimization techniques<br /><br />K-means optimization objective<br />K - total no of clusters <br />k = {1,2,3,....K}<br /><br />The cost function that k-means is minimizing is a function J of all of these parameters, c1 through cm and mu 1 through mu K. That k-means is varying as the algorithm runs. And the optimization objective is shown to the right, is the average of 1 over m of sum from i equals 1 through m of this term here.<br /><br />Trying to find c and mu to try to minimize this cost function J. This cost function is sometimes also called the distortion cost function, or the distortion of the k-means algorithm.<br /><br /><a href=""><img src="images/22-11.png" alt="images/22-11.png" /></a><br /><br /><br />Cluster assignment step - assign each point to the closest centroid <br />the cluster assignment step does is it doesn&#39;t change the cluster centroids, but what it&#39;s doing is this is exactly picking the values of c1, c2, up to cm. That minimizes the cost function, or the distortion function J.<br />The second step : K means was the move centroid step.<br />taking the two sets of variables and partitioning them into two halves right here. First the c sets of variables and then you have the mu sets of variables. And what it does is it first minimizes J with respect to the variable c and then it minimizes J with respect to the variables mu and then it keeps on. <br />We can use that to debug k-means and help make sure that k-means is converging and is running properly.<br /><a href=""><img src="images/22-12.png" alt="images/22-12.png" /></a><br /><br /><br />Random Initialization <br /> <br />how to make K-means avoid local optima as well.<br />m-training examples<br />  <br /><a href=""><img src="images/22-13.png" alt="images/22-13.png" /></a><br /><br /><a href=""><img src="images/22-14.png" alt="images/22-14.png" /></a><br /><br /> instead of just initializing K-means once and hopping that that works, what we can do is, initialize K-means lots of times and run K-means lots of times, and use that to try to make sure we get as good a solution, as good a local or global optima as possible.<br /><br /> <br />You will have a hundred different ways of clustering the data and then finally what you do is all of these hundred ways you have found of clustering the data, just pick one, that gives us the lowest cost. <br />That gives us the lowest distortion. And it turns out that if you are running K-means with a fairly small number of clusters , so you know if the number of clusters is anywhere from two up to maybe 10 - then doing multiple random initializations can often, can sometimes make sure that you find a better local optima. Make sure you find the better clustering data. But if K is very large, so, if K is much greater than 10, certain if K were, you know, if you were trying to find hundreds of clusters, then,having multiple random initializations is less likely to make a huge difference and there is a much higher chance that your first random initialization will give you a pretty decent solution already<br /><br /><a href=""><img src="images/22-15.png" alt="images/22-15.png" /></a><br /><br />Choosing the number of clusters<br /><a href=""><img src="images/22-16.png" alt="images/22-16.png" /></a><br /> the Elbow Method, what we&#39;re going to do is vary K, which is the total number of clusters. So, we&#39;re going to run K-means with one cluster, that means really, everything gets grouped into a single cluster and compute the cost function or compute the distortion J and plot that here. And then we&#39;re going to run K means with two clusters, maybe with multiple random initial agents, maybe not. But then, you know, with two clusters we should get, hopefully, a smaller distortion, and so plot that there. <br />And then run K-means with three clusters, hopefully, you get even smaller distortion and plot that there. <br />I&#39;m gonna run K-means with four, five and so on.<br /><br /><br />if you actually do this in a practice, you know, if your plot looks like the one on the left and that&#39;s great. <br />It gives you a clear answer, but just as often, you end up with a plot that looks like the one on the right and is not clear where the ready location of the elbow is. It makes it harder to choose a number of clusters using this method. So maybe the quick summary of the Elbow Method is that is worth the shot but I wouldn&#39;t necessarily,you know, have a very high expectation of it working for any particular problem.<br /><br /><a href=""><img src="images/22-17.png" alt="images/22-17.png" /></a><br /><br /><a href=""><img src="images/22-18.png" alt="images/22-18.png" /></a><br /><br /><br /><br />Motivation	1 : Data compression<br />Dementionality reduction<br />second type of unsupervised learning problem called dimensionality reduction.<br /> data compression not only allows us to compress the data and have it therefore use up less computer memory or disk space, but it will also allow us to speed up our learning algorithms<br /><br />if the length in centimeters were rounded off to the nearest centimeter and lengthened inches was rounded off to the nearest inch. Then, that&#39;s why these examples <br />don&#39;t lie perfectly on a straight line, because of, you know, round-off error to the nearest centimeter or the nearest inch. <br />And if we can reduce the data to one dimension instead of two dimensions, that reduces the redundancy.<br /><a href=""><img src="images/22-19.png" alt="images/22-19.png" /></a><br /><br /><br /> if you highly correlated features, maybe you really want to reduce the dimension<br /> <br /> <a href=""><img src="images/22-20.png" alt="images/22-20.png" /></a><br /> <br />this is an approximation to the original training self because I have projected all of my training examples onto a line<br /><br /><a href=""><img src="images/22-21.png" alt="images/22-21.png" /></a><br /><br /><br /><a href=""><img src="images/22-22.png" alt="images/22-22.png" /></a><br /><br /><br />Motivation II : Data Visualization<br /><a href=""><img src="images/22-23.png" alt="images/22-23.png" /></a><br /><br /><a href=""><img src="images/22-24.png" alt="images/22-24.png" /></a><br /><br /><a href=""><img src="images/22-25.png" alt="images/22-25.png" /></a><br /><br /><br /><br />Principal Component Analysis Problem Formulation<br />PCA<br />For the problem of dimensionality reduction, by far the most popular, by far the most commonly used algorithm is something called <br />principle components analysis, or PCA<br /><br />The length of those blue line segments, that&#39;s sometimes also called the projection error. <br />And so what PCA does is it tries to find a surface onto which to project the data so as to minimize that.<br /><br />it&#39;s standard practice to first perform mean normalization at feature scaling so that the features x1 and <br />x2 should have zero mean, and should have comparable ranges of values.<br /><br /><a href=""><img src="images/22-26.png" alt="images/22-26.png" /></a><br />where the PCA gives me u(1) or -u(1), doesn&#39;t matter. positive vector or negative vector<br /><br /><a href=""><img src="images/22-27.png" alt="images/22-27.png" /></a><br /><br />if you&#39;re not familiar with linear algebra, just think of it as <br />finding k directions instead of just one direction onto which to project the data. <br />So finding a k-dimensional surface is really finding a 2D plane in this case, <br />shown in this figure, <br />where we can define the position of the points in a plane using k directions. <br />And that&#39;s why for PCA we want to find k vectors onto which to project the data. <br />And so more formally in PCA, what we want to do is find this way to project the data <br />so as to minimize the sort of projection distance, <br />which is the distance between the points and the projections. <br />And so in this 3D example too. <br />Given a point we would take the point and project it onto this 2D surface.<br />We are done with that. <br />And so the projection error would be, the distance between the point and <br />where it gets projected down to my 2D surface. <br />And so what PCA does is I try to find the line, or a plane, or whatever, <br />onto which to project the data, to try to minimize that square projection, <br />that 90 degree or that orthogonal projection error. <br /><br /><br />If we were doing linear regression, what we would do would be, on the left we would <br />be trying to predict the value of some variable y given some info features x. <br />And so linear regression, what we&#39;re doing is we&#39;re fitting a straight line so <br />as to minimize the square error between point and this straight line. <br />And so what we&#39;re minimizing would be the squared magnitude of these blue lines. <br />And notice that I&#39;m drawing these blue lines vertically. <br />That these blue lines are the vertical distance between the point and <br />the value predicted by the hypothesis. <br />Whereas in contrast, in PCA, what it does is it tries to minimize <br />the magnitude of these blue lines, which are drawn at an angle. <br />These are really the shortest orthogonal distances. <br />The shortest distance between the point x and this red line.<br /><br />there is no special variable y in PCA, every x1,x2, are treated the same<br /><a href=""><img src="images/22-28.png" alt="images/22-28.png" /></a><br /><br />if I have three-dimensional data and <br />I want to reduce data from 3D to 2D, so maybe I wanna find two directions, <br />u(1) and u(2), onto which to project my data. <br />Then what I have is I have three features, x1, x2, x3, and <br />all of these are treated alike. <br />All of these are treated symmetrically and <br />there&#39;s no special variable y that I&#39;m trying to predict.<br /><br /><a href=""><img src="images/22-29.png" alt="images/22-29.png" /></a><br /><br /><br /><br />Principal Component Analysis algorithms <br /><br />Before applying PCA, there is a data pre-processing step which you should always do. <br />Given the trading sets of the examples is important to always perform mean normalization,<br />and then depending on your data, maybe perform feature scaling as well.<br />this is very similar to the mean normalization and feature scaling <br />process that we have for supervised learning.<br /><br /><br />In fact it&#39;s exactly the same procedure except that we&#39;re doing it now to our unlabeleddata, X1 through Xm. <br />So for mean normalization we first compute the mean of each feature and then <br />we replace each feature, X, with X minus its mean, and so this makes each feature now have exactly zero mean.<br /><br />Similar to what we had with supervised learning, we would take x, i substitute j, that&#39;s the j feature<br /><br />and so we would subtract of the mean, now that&#39;s what we have on top, and then divide by sj. <br />Here, sj is some measure of the beta values of feature j. So, it could be the max minus min value, or more commonly, <br />it is the standard deviation of feature j. Having done this sort of data pre-processing, here&#39;s what the PCA algorithm does.<br /><a href=""><img src="images/22-30.png" alt="images/22-30.png" /></a><br /><br />compute the vectors u(i) and compute the z values<br /><a href="https://www.youtube.com/watch?v=FgakZw6K1QQ">https://www.youtube.com/watch?v=FgakZw6K1QQ</a> StatQuest on PCA<br /><br /><a href=""><img src="images/22-31.png" alt="images/22-31.png" /></a><br /><br /><a href=""><img src="images/22-32.png" alt="images/22-32.png" /></a><br /><br />It turns out that the SVD function and the I function it will give <br />you the same vectors, although SVD is a little more numerically stable. <br />So I tend to use SVD, although I have a few friends that use the I function to do this as wellbut when you apply this to a covariance matrix <br />sigma it gives you the same thing. <br />This is because the covariance matrix always satisfies a mathematical Property called symmetric positive definite <br />You really don&#39;t need to know what that means, but the SVDand I-functions are different functions but when they are applied to a <br />covariance matrix which can be proved to always satisfy this mathematical property; they&#39;ll always give you the same thing.<br /><a href=""><img src="images/22-33.png" alt="images/22-33.png" /></a><br /><br />from the SVD numerical linear algebra routine we get these matrices u, s, <br />and d. we&#39;re going to use the first K columns of this matrix to get u1-uK.<br />Now the other thing we need to is take my original data set, X which is an RN And find a lower dimensional representation Z, which is a R K for this data. <br />So the way we&#39;re going to do that is take the first K Columns of the U matrix.<br />Construct this matrix.<br />Stack up U1, U2 andso on up to U K in columns. <br />It&#39;s really basically taking, you know, this part of the matrix, the first K columns of this matrix.<br /><br />And so this is going to be an N by K matrix. <br />I&#39;m going to give this matrix a name. <br />I&#39;m going to call this matrix U, subscript &quot;reduce,&quot; sort of a reduced version of the U matrix maybe. <br />I&#39;m going to use it to reduce the dimension of my data.<br /><br />And the way I&#39;m going to compute Z is going to let Z be equal to this U reduce matrix transpose times X. Or alternatively, you know, <br />to write down what this transpose means. When I take this transpose of this U matrix, what I&#39;m going to end up with is these vectors now in rows. I have U1 transpose down to UK transpose.<br />Then take that times X, and that&#39;s how I get my vector Z<br /><br /><a href=""><img src="images/22-34.png" alt="images/22-34.png" /></a><br /><a href=""><img src="images/22-35.png" alt="images/22-35.png" /></a><br /><br /><br />Applying PCA<br />Reconstruction from compressed Representation<br /><br /><a href=""><img src="images/22-36.png" alt="images/22-36.png" /></a><br /><br /><br />Choosing the number of principal components<br /><br />In the PCA algorithm we take N dimensional features and reduce them to some K dimensional feature representation.<br />This number K is a parameter of the PCA algorithm.<br />This number K is also called the number of principle components or the number of principle components that we&#39;ve retained.<br /><br /><a href=""><img src="images/22-37.png" alt="images/22-37.png" /></a><br /><br /><a href=""><img src="images/22-38.png" alt="images/22-38.png" /></a><br /><br />And by the way, even if you were to pick some different value of K, even if you were to pick the value of K <br />manually, you know maybe you have a thousand dimensional data and I just want to choose K equals one hundred. <br />Then, if you want to explain to others what you just did, a good way to explain the performance <br />of your implementation of PCA to them, is actually to take this quantity and compute what this is, and that will tell you what was the percentage of variance retained. <br />And if you report that number, then, you know, people that are familiar with PCA, and people can use this to get a good understanding of how well your hundred dimensional representation is approximating your original data set, because there&#39;s 99% of variance retained. <br />That&#39;s really a measure of your square of construction error, that ratio being 0.01, just gives people a good intuitive <br />sense of whether your implementation of PCA is finding a good approximation of your original data set.<br /><br /><a href=""><img src="images/22-39.png" alt="images/22-39.png" /></a><br /><br />Advice for applying PCA<br /><br />Here&#39;s how you can use PCA to speed up a learning algorithm, and this supervised learning algorithm speed up is actually the most common use that I personally make of PCA<br /><br />One final note, what PCA does is it defines a mapping from x to z and this mapping from x to z should be defined by running <br />PCA only on the training sets. And in particular, this mapping that <br />PCA is learning, right, this mapping, what that does is it computes the set of parameters. <br />That&#39;s the feature scaling and mean normalization. And there&#39;s also computing this matrix U reduced. But all of these things that U reduce, that&#39;s like a parameter that is learned by PCA and we should be fitting our parameters only to our training sets and not to our cross validation or test sets and so these things the U reduced <br />so on, that should be obtained by running PCA only on your training set. <br />And then having found U reduced, or having found the parameters for feature scaling where the mean normalization <br />and scaling the scale that you divide the features by to get them on to comparable scales. <br />Having found all those parameters on the training set, you can then apply the same mapping to other examples that may be <br />In your cross-validation sets or in your test sets.<br /><br /><a href=""><img src="images/22-40.png" alt="images/22-40.png" /></a><br /><br /><a href=""><img src="images/22-41.png" alt="images/22-41.png" /></a><br /><br /><a href=""><img src="images/22-42.png" alt="images/22-42.png" /></a><br /><br /><a href=""><img src="images/22-43.png" alt="images/22-43.png" /></a><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /></div></body></html>