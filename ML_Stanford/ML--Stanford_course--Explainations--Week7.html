<!doctype html><html>
    <head>
      <meta http-equiv="content-type" content="text/html; charset=utf-8">
      <title>Week7</title>
      <meta name="generator" content="CherryTree">
      <link rel="stylesheet" href="res/styles3.css" type="text/css" />
    </head>
    <body><div class='page'><h1 class='title'>Week7</h1><br/>we have large margin classification, kernals, SVMs in practice<br /><br />SVM - support vector machine<br />Optimization Objective<br /><br />amount of data , algorithms used <br />very powerful algo - support vector machine  - cleaner in terms of nonlinear functions<br /><a href=""><img src="images/21-1.png" alt="images/21-1.png" /></a><br /><br /><br />In case of y=0 ,<br />The purple curve is pretty close appox of the sigmoid curve shown<br />this purple curve gives a logistic regression and helps alot with the SVM <br /><a href=""><img src="images/21-2.png" alt="images/21-2.png" /></a><br /><br />for svm we paramertized sightly differently, at lambda<br /><a href=""><img src="images/21-3.png" alt="images/21-3.png" /></a><br /><br /><br />unlike logistic regression, the support vector machine doesn&#39;t output the probability is that what we have is we have this cost function, that we minimize to get the parameter&#39;s data, and what a support vector machine does is it just makes a prediction of y being equal to one or zero, directly. So the hypothesis will predict one if theta transpose x is greater or equal to zero, and it will predict zero otherwise and so having learned the parameters theta, this is the form of the hypothesis for the support vector machine.<br /><br /><br /><br />Large Margin Intuition<br /><a href=""><img src="images/21-4.png" alt="images/21-4.png" /></a><br /><br />set C as very large value <br /><br /><a href=""><img src="images/21-5.png" alt="images/21-5.png" /></a><br /><br />how does this large margin classifier end from the above formula<br /><a href=""><img src="images/21-6.png" alt="images/21-6.png" /></a><br /><br /><a href=""><img src="images/21-7.png" alt="images/21-7.png" /></a><br /> I hope that gives some intuition about how this support vector machine functions as  large margin classifier that tries to separate the data with a large margin, technically this picture of this view is true only when the parameter C is very large, which isa useful way to think about support vector machines<br /><br />Mathematics behind large margin classification<br />inner product <br /><a href=""><img src="images/21-8.png" alt="images/21-8.png" /></a><br /><a href=""><img src="images/21-9.png" alt="images/21-9.png" /></a><br /><br />|| theta || is norm of theta<br /><a href=""><img src="images/21-10.png" alt="images/21-10.png" /></a><br /><br /><br />Kernel I<br /><br />adapting the SVMs in order to develop complex nonlinear classifiers<br />main technique is use kernels<br /><a href=""><img src="images/21-11.png" alt="images/21-11.png" /></a><br /><br />Define new examples as follows :<br /> gaussian kernel formula <br /> <br /><a href=""><img src="images/21-12.png" alt="images/21-12.png" /></a><br />l- landmark<br />the question is, is there a different choice of features or is there better sort of features than this high order polynomials because you know it&#39;s not clear that this high order polynomial is what we want, and what we talked about computer vision talk about when the input is an image with lots of pixels. <br />We also saw how using high order polynomials becomes very computationally expensive because there are a lot of these higher order polynomial terms.<br />So, is there a different or a better choice of the features that we can use to plug into this sort of hypothesis form<br /><br />So what these features do is they measure how similar X is from one of your landmarks and the feature f is going to be close to one when X is close to your landmark and is going to be 0 or close to zero when X is far from your landmark.<br />Each of these landmarks, defines a new feature f1, f2 and f3. That is, given the the training example X, we can now compute three new features: f1, f2, and f3, given, you know, the three landmarks that I wrote just now. <br />But first, let&#39;s look at this exponentiation function, let&#39;s look at this similarity function and plot in some figures and just, you know, understand better what this really looks like.<br /><br /><a href=""><img src="images/21-13.png" alt="images/21-13.png" /></a><br /><br />Now the other was due on this slide is show the effects of varying this parameter sigma squared. So, sigma squared is the parameter of the Gaussian kernel and as you vary it, you get slightly different effects.<br />Let&#39;s set sigma squared to be equal to 0.5 and see what we get. We set sigma square to 0.5, what you find is that the kernel looks similar, except for the width of the bump becomes narrower. <br />The contours shrink a bit too. So if sigma squared equals to 0.5 then as you start from X equals 3 5 and as you move away,then the feature f1 falls to zero much more rapidly and conversely.And if sigma squared is large, then as you move away from l1, the value of the feature falls away much more slowly.<br /><a href=""><img src="images/21-14.png" alt="images/21-14.png" /></a><br /><br /><a href=""><img src="images/21-15.png" alt="images/21-15.png" /></a><br /><br />We can learn pretty complex non-linear decision boundary, like what I just drew where we predict positive when we&#39;re close to either one of the two landmarks. And we predict negative when we&#39;re very far away from any of the landmarks. And so this is part of the idea of kernels of and how we use them with the support vector machine, which is that we define these extra features using landmarks and similarity functions to learn more complex nonlinear classifiers.<br /><br /><br /><br />Kernel II<br /><br />yes how do we get landmarks<br /><a href=""><img src="images/21-16.png" alt="images/21-16.png" /></a><br /><br />similarities<br /><a href=""><img src="images/21-17.png" alt="images/21-17.png" /></a><br /><br /><br /> this is the idea here which is that we&#39;re gonna take the examples and for every training example that we have, we are just going to call it. We&#39;re just going to put landmarks as exactly the same locations as the training examples<br /><br />When you are given example x, and in this example x can be something in the training set, it can be something in the cross validation set, or it can be something in the test set. Given an example x we are going to compute, you know, these features as so f1, f2, and so on. Where l1 is actually equal to x1 and so on. And these then give me a feature vector. So let me write f as the feature vector. I&#39;m going to take these f1, f2 and so on, and just group them into feature vector<br /><br />Depending on whether you can set terms, is either R(n) or R(n) plus 1. We can now instead represent my <br />training example using this feature vector f. I am <br />going to write this f superscript i. Which is going to be taking all of these things and stacking them into a vector.So, f1(i) down to fm(i) and if you want and well, usually we&#39;ll also add this f0(i), where f0(i) is equal to 1. And so this vector here gives me my new feature vector with which to represent my training example. So given these kernels and similarity functions, here&#39;s how we use a simple vector machine<br /><br /><br />So that&#39;s how you make a prediction if you already have a setting for the parameter&#39;s theta. How do you get the parameter&#39;s theta? Well you do that using the SVM learning algorithm, and specifically what you do is you would solve this minimization problem. You&#39;ve minimized the parameter&#39;s theta of C times this cost function which we had before. <br />Only now, instead of looking there instead of making predictions using theta transpose x(i) using our original features, x(i). Instead we&#39;ve taken the features x(i) and replace them with a new features<br /><br /><br />And what most support vector machine implementations do is actually replace this theta transpose theta, will instead, theta transpose times some matrix inside, that depends on the kernel you use, times theta. And so this gives us a slightly different distance metric. We&#39;ll use a slightly different measure instead of minimizing exactly the norm of theta squared means that minimize something slightly similar to it. That&#39;s like a rescale version of the parameter vector theta that depends on the kernel. But this is kind of a mathematical detail. <br />That allows the support vector machine software to run much more efficiently.<br />And the reason the support vector machine does this is with this modification. It allows it to scale to much bigger training sets. Because for example, if you have a training set with 10,000 training examples.<br /><br /><a href=""><img src="images/21-18.png" alt="images/21-18.png" /></a><br /><br />bias and variance trade off<br /><br />When using an SVM, one of the things you need to choose is the parameter C which was in the optimization objective, and you recall that C played a role similar to 1 over lambda, where lambda was the regularization parameter we had for logistic regression.So, if you have a large value of C, this corresponds to what we have back in logistic regression, of a small value of lambda meaning of not using much regularization.  if you do that, you tend to have a hypothesis with lower bias and higher variance.<br /><br />Whereas if you use a smaller  of C then this corresponds to when we are using logistic regression with a large value of lambda and that corresponds to a hypothesis with higher bias and lower variance. And so, hypothesis with large C has a higher variance, and is more prone to overfitting, whereas hypothesis with small C has higher bias and is thus more prone to underfitting.<br /><br /><a href=""><img src="images/21-19.png" alt="images/21-19.png" /></a><br /><br /><br />Using an SVM<br /><a href=""><img src="images/21-20.png" alt="images/21-20.png" /></a><br /><a href=""><img src="images/21-21.png" alt="images/21-21.png" /></a><br /><a href=""><img src="images/21-22.png" alt="images/21-22.png" /></a><br /><br /><a href=""><img src="images/21-23.png" alt="images/21-23.png" /></a><br /><br /><a href=""><img src="images/21-24.png" alt="images/21-24.png" /></a><br /><a href=""><img src="images/21-25.png" alt="images/21-25.png" /></a><br /><br /><br /><br /><br /><br /></div></body></html>